{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as ds\n",
    "from torchvision import models, transforms, utils, datasets\n",
    "from torch.utils import data\n",
    "from torchsummary import summary\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in Is_Fish Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce RTX 3060 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "model_ft.load_state_dict(torch.load(\"models/is_fish.pt\"))\n",
    "model_ft.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idea for Workflow\n",
    "- Use Fishbase as a test and validation set\n",
    "- Build an initial proof of concept classifier for species that have enough data in FishBase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Fishbase Dataset\n",
    "### Determine Fishbase Test and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_df = pd.read_csv(\"data/fish_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = {}\n",
    "fishbase_source_path = \"data/fishbase_images/\"\n",
    "for species_dir in os.listdir(fishbase_source_path):\n",
    "    if species_dir == \".ipynb_checkpoints\":\n",
    "        continue\n",
    "    image_count = len([name for name in os.listdir(fishbase_source_path + species_dir + \"/\") if name.split(\".\")[-1] != \"gif\"])\n",
    "    #print(f\"{species_dir}: {image_count}\")\n",
    "    sizes[species_dir] = image_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(sizes.values())\n",
    "plt.title(\"File Count Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "# Find the number of species with a minimum of 25  images from fishbase\n",
    "thresh = 25\n",
    "mtt_species= []\n",
    "for species, count in sizes.items():\n",
    "    if count > thresh:\n",
    "        mtt_species.append(species)\n",
    "print(len(mtt_species))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Scraped Image Counts for the Species that Meet IsFish Threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([transforms.ToTensor(), transforms.Resize((224, 224)), ]) #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to parse a given directory and find the number of fish vs nonfish images\n",
    "def count_fish(species_path):\n",
    "    fish_count = 0\n",
    "    not_fish_count = 0\n",
    "    deltas = []\n",
    "    image_paths = []\n",
    "    for file in os.listdir(species_path):\n",
    "        image_path = species_path + file\n",
    "        try:\n",
    "            img = Image.open(image_path).convert('RGB')\n",
    "        except:\n",
    "            continue\n",
    "        img_filt = data_transforms(img).cuda().unsqueeze(0)\n",
    "        outputs = model_ft(img_filt)\n",
    "\n",
    "        _, preds =torch.max(outputs, 1)\n",
    "        preds = preds.tolist()\n",
    "        fish_score = outputs.tolist()[0][0]\n",
    "        not_fish_score = outputs.tolist()[0][1]\n",
    "        delta = fish_score - not_fish_score \n",
    "        preds = preds[0]\n",
    "        # 1 is for not fish and 0 is for fish\n",
    "        mod_pred = delta > -2\n",
    "\n",
    "        if mod_pred == 1:\n",
    "            fish_count += 1\n",
    "            image_paths.append(image_path)\n",
    "\n",
    "        else:\n",
    "            not_fish_count += 1 \n",
    "            deltas.append(delta)\n",
    "            \n",
    "    return fish_count, not_fish_count, image_paths\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shivaram/anaconda3/envs/pytorch/lib/python3.9/site-packages/PIL/Image.py:945: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Find scientific image count by species\n",
    "sci_path = \"/media/shivaram/SharedVolum/Projects/FishID/scraped_images/scientific/\" \n",
    "scientific_counts = dict()\n",
    "for species_id in mtt_species:\n",
    "    try:\n",
    "        species_path = sci_path + species_id + \"/\"\n",
    "        ic, _, _ = count_fish(species_path)\n",
    "    except:\n",
    "        ic = 0\n",
    "    scientific_counts[species_id] = ic\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find common image count by species\n",
    "common_path = \"/media/shivaram/SharedVolum/Projects/FishID/scraped_images/regular/\" \n",
    "common_counts = dict()\n",
    "for species_id in mtt_species:\n",
    "    try:\n",
    "        species_path = common_path + species_id + \"/\"\n",
    "        ic, _, _ = count_fish(species_path)\n",
    "    except:\n",
    "        ic = 0\n",
    "    common_counts[species_id] = ic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Initial Proof of Concept Dataset\n",
    "I decided that I would build an initial proof of concept model on species that had more data before expanding to ones with less. For this model, I decided to limit myself to species that had 25+ images in Fishbase and 50+ images scraped from the common name and the scientific name each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    }
   ],
   "source": [
    "# Find fish classes that have enough train data (50+ in scientific and common)\n",
    "all_thresh_ids = []\n",
    "train_thresh = 50\n",
    "for fid in mtt_species:\n",
    "    if common_counts[fid] > train_thresh and scientific_counts[fid] > train_thresh:\n",
    "        all_thresh_ids.append(fid)\n",
    "print(len(all_thresh_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Symlink Directories for Train, Test, Val\n",
    "I am not sure how the model performance will change based on the training data, so I am trying to train it with three different training sets: once with scientific web scraped images, once with the common name web scraped images, and once with both combined. To do this however, I need to create training folder structures, which I do below via symlinks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories needed for data\n",
    "if not os.path.isdir(\"/home/shivaram/DS/Projects/FishID/data/expanded_model_data/is_fish_scientific/\"):\n",
    "    for subfolder in [\"train/\", \"test/\", \"val/\"]:\n",
    "        for sid in all_thresh_ids:\n",
    "            os.makedirs(\"/home/shivaram/DS/Projects/FishID/data/expanded_model_data/is_fish_scientific/\" + subfolder + sid + \"/\")\n",
    "\n",
    "if not os.path.isdir(\"/home/shivaram/DS/Projects/FishID/data/expanded_model_data/is_fish_common/\"):\n",
    "    for subfolder in [\"train/\", \"test/\", \"val/\"]:\n",
    "        for sid in all_thresh_ids:\n",
    "            os.makedirs(\"/home/shivaram/DS/Projects/FishID/data/expanded_model_data/is_fish_common/\" + subfolder + sid + \"/\")\n",
    "\n",
    "\n",
    "if not os.path.isdir(\"/home/shivaram/DS/Projects/FishID/data/expanded_model_data/is_fish_mixed/\"):\n",
    "    for subfolder in [\"train/\", \"test/\", \"val/\"]:\n",
    "        for sid in all_thresh_ids:\n",
    "            os.makedirs(\"/home/shivaram/DS/Projects/FishID/data/expanded_model_data/is_fish_mixed/\" + subfolder + sid + \"/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 27/379 for 914 because it was already ported from the scientific dataset\n",
      "Skipped 111/366 for 6024 because it was already ported from the scientific dataset\n",
      "Skipped 7/500 for 6468 because it was already ported from the scientific dataset\n",
      "Skipped 61/500 for 977 because it was already ported from the scientific dataset\n",
      "Skipped 125/384 for 7293 because it was already ported from the scientific dataset\n",
      "Skipped 118/426 for 6630 because it was already ported from the scientific dataset\n",
      "Skipped 99/355 for 5598 because it was already ported from the scientific dataset\n",
      "Skipped 76/497 for 2467 because it was already ported from the scientific dataset\n",
      "Skipped 97/380 for 1260 because it was already ported from the scientific dataset\n",
      "Skipped 17/347 for 3385 because it was already ported from the scientific dataset\n",
      "Skipped 58/442 for 5950 because it was already ported from the scientific dataset\n",
      "Skipped 53/363 for 1005 because it was already ported from the scientific dataset\n",
      "Skipped 64/319 for 4274 because it was already ported from the scientific dataset\n",
      "Skipped 45/368 for 5584 because it was already ported from the scientific dataset\n",
      "Skipped 138/377 for 5952 because it was already ported from the scientific dataset\n",
      "Skipped 10/378 for 239 because it was already ported from the scientific dataset\n",
      "Skipped 4/206 for 1007 because it was already ported from the scientific dataset\n",
      "Skipped 63/378 for 3 because it was already ported from the scientific dataset\n",
      "Skipped 111/385 for 1309 because it was already ported from the scientific dataset\n",
      "Skipped 17/499 for 143 because it was already ported from the scientific dataset\n",
      "Skipped 38/315 for 753 because it was already ported from the scientific dataset\n",
      "Skipped 50/392 for 93 because it was already ported from the scientific dataset\n",
      "Skipped 112/369 for 4485 because it was already ported from the scientific dataset\n",
      "Skipped 92/366 for 2534 because it was already ported from the scientific dataset\n",
      "Skipped 80/378 for 1022 because it was already ported from the scientific dataset\n",
      "Skipped 48/391 for 1002 because it was already ported from the scientific dataset\n",
      "Skipped 122/416 for 4905 because it was already ported from the scientific dataset\n",
      "Skipped 5/97 for 1895 because it was already ported from the scientific dataset\n",
      "Skipped 67/500 for 6556 because it was already ported from the scientific dataset\n",
      "Skipped 139/367 for 1265 because it was already ported from the scientific dataset\n",
      "Skipped 54/380 for 4659 because it was already ported from the scientific dataset\n",
      "Skipped 34/377 for 4684 because it was already ported from the scientific dataset\n",
      "Skipped 33/356 for 998 because it was already ported from the scientific dataset\n",
      "Skipped 65/328 for 5791 because it was already ported from the scientific dataset\n",
      "Skipped 69/394 for 2576 because it was already ported from the scientific dataset\n",
      "Skipped 166/492 for 5557 because it was already ported from the scientific dataset\n",
      "Skipped 40/373 for 875 because it was already ported from the scientific dataset\n",
      "Skipped 69/378 for 868 because it was already ported from the scientific dataset\n",
      "Skipped 119/380 for 5444 because it was already ported from the scientific dataset\n",
      "Skipped 132/352 for 5824 because it was already ported from the scientific dataset\n",
      "Skipped 51/360 for 785 because it was already ported from the scientific dataset\n",
      "Skipped 44/348 for 1450 because it was already ported from the scientific dataset\n",
      "Skipped 78/401 for 5631 because it was already ported from the scientific dataset\n",
      "Skipped 97/374 for 10276 because it was already ported from the scientific dataset\n",
      "Skipped 15/436 for 6555 because it was already ported from the scientific dataset\n",
      "Skipped 0/365 for 89 because it was already ported from the scientific dataset\n",
      "Skipped 19/360 for 898 because it was already ported from the scientific dataset\n",
      "Skipped 38/491 for 874 because it was already ported from the scientific dataset\n",
      "Skipped 42/354 for 3231 because it was already ported from the scientific dataset\n",
      "Skipped 4/413 for 861 because it was already ported from the scientific dataset\n",
      "Skipped 52/85 for 1750 because it was already ported from the scientific dataset\n",
      "Skipped 82/295 for 387 because it was already ported from the scientific dataset\n",
      "Skipped 34/337 for 4278 because it was already ported from the scientific dataset\n",
      "Skipped 24/80 for 4914 because it was already ported from the scientific dataset\n",
      "Skipped 129/459 for 5606 because it was already ported from the scientific dataset\n",
      "Skipped 128/395 for 6401 because it was already ported from the scientific dataset\n",
      "Skipped 77/379 for 1258 because it was already ported from the scientific dataset\n",
      "Skipped 63/370 for 907 because it was already ported from the scientific dataset\n",
      "Skipped 42/193 for 5425 because it was already ported from the scientific dataset\n",
      "Skipped 32/400 for 107 because it was already ported from the scientific dataset\n",
      "Skipped 22/399 for 5562 because it was already ported from the scientific dataset\n",
      "Skipped 112/370 for 1261 because it was already ported from the scientific dataset\n",
      "Skipped 38/350 for 80 because it was already ported from the scientific dataset\n",
      "Skipped 91/322 for 5623 because it was already ported from the scientific dataset\n",
      "Skipped 108/352 for 5398 because it was already ported from the scientific dataset\n",
      "Skipped 91/380 for 5891 because it was already ported from the scientific dataset\n",
      "Skipped 48/344 for 5596 because it was already ported from the scientific dataset\n",
      "Skipped 58/378 for 988 because it was already ported from the scientific dataset\n",
      "Skipped 123/381 for 1869 because it was already ported from the scientific dataset\n",
      "Skipped 100/456 for 1732 because it was already ported from the scientific dataset\n",
      "Skipped 56/379 for 412 because it was already ported from the scientific dataset\n",
      "Skipped 6/425 for 271 because it was already ported from the scientific dataset\n",
      "Skipped 19/399 for 2081 because it was already ported from the scientific dataset\n",
      "Skipped 9/265 for 886 because it was already ported from the scientific dataset\n",
      "Skipped 10/352 for 226 because it was already ported from the scientific dataset\n",
      "Skipped 59/363 for 912 because it was already ported from the scientific dataset\n",
      "Skipped 137/495 for 6025 because it was already ported from the scientific dataset\n",
      "Skipped 22/388 for 1077 because it was already ported from the scientific dataset\n",
      "Skipped 29/379 for 6504 because it was already ported from the scientific dataset\n",
      "Skipped 62/345 for 917 because it was already ported from the scientific dataset\n",
      "Skipped 17/374 for 5553 because it was already ported from the scientific dataset\n",
      "Skipped 52/360 for 3276 because it was already ported from the scientific dataset\n",
      "Skipped 51/409 for 343 because it was already ported from the scientific dataset\n",
      "Skipped 49/376 for 6 because it was already ported from the scientific dataset\n",
      "Skipped 163/366 for 1235 because it was already ported from the scientific dataset\n",
      "Skipped 87/500 for 12600 because it was already ported from the scientific dataset\n",
      "Skipped 36/221 for 1906 because it was already ported from the scientific dataset\n",
      "Skipped 9/500 for 3228 because it was already ported from the scientific dataset\n",
      "Skipped 17/378 for 5923 because it was already ported from the scientific dataset\n",
      "Skipped 129/360 for 5645 because it was already ported from the scientific dataset\n",
      "Skipped 20/404 for 4275 because it was already ported from the scientific dataset\n",
      "Skipped 27/259 for 5839 because it was already ported from the scientific dataset\n"
     ]
    }
   ],
   "source": [
    "source_data_path =  \"/media/shivaram/SharedVolum/Projects/FishID/scraped_images/\"\n",
    "fishbase_source_path = \"/media/shivaram/SharedVolum/Projects/FishID/data/fishbase_images/\"\n",
    "\n",
    "save_dirs = [\"/home/shivaram/DS/Projects/FishID/data/expanded_model_data/is_fish_scientific/\", \"/home/shivaram/DS/Projects/FishID/data/expanded_model_data/is_fish_common/\", \"/home/shivaram/DS/Projects/FishID/data/expanded_model_data/is_fish_mixed/\"]\n",
    "\n",
    "random.seed(10)\n",
    "\n",
    "count = 0\n",
    "done = False \n",
    "for root_dir, cur_dir, files in os.walk(\"/home/shivaram/DS/Projects/FishID/data/expanded_model_data/is_fish_mixed/\"):\n",
    "    if len(files) > 0:\n",
    "        done = True\n",
    "\n",
    "        \n",
    "if not done:\n",
    "    for species_id in all_thresh_ids:\n",
    "\n",
    "        # Create validation and test sets from FishBase\n",
    "        for file in os.listdir(fishbase_source_path + species_id + \"/\"):\n",
    "            image_path = fishbase_source_path + species_id + \"/\" + file\n",
    "\n",
    "            # Create test and validation datasets (same for each of the different datasets)\n",
    "            test_val = bool(random.getrandbits(1))\n",
    "            if test_val: \n",
    "                save_folder = \"test/\"\n",
    "            else:\n",
    "                save_folder = \"val/\" \n",
    "            for save_dir in save_dirs:\n",
    "                save_path = save_dir + save_folder +  species_id + \"/\" + file \n",
    "                os.symlink(image_path, save_path)\n",
    "                \n",
    "        # Build scientific based dataset (and mixed)\n",
    "        for file in os.listdir(source_data_path + \"scientific/\" + species_id + \"/\"):\n",
    "            image_path  = source_data_path + \"scientific/\" + species_id + \"/\" + file\n",
    "\n",
    "            savepath_sci = \"/home/shivaram/DS/Projects/FishID/data/expanded_model_data/is_fish_scientific/train/\" + species_id + \"/\" + file\n",
    "            savepath_mixed = \"/home/shivaram/DS/Projects/FishID/data/expanded_model_data/is_fish_mixed/train/\" + species_id + \"/\" + file\n",
    "\n",
    "            os.symlink(image_path, savepath_sci)\n",
    "            os.symlink(image_path, savepath_mixed)\n",
    "\n",
    "\n",
    "        # Build common name dataset (and mixed)\n",
    "        skipped = 0\n",
    "        fc = len(os.listdir(source_data_path + \"regular/\" + species_id + \"/\"))\n",
    "        \n",
    "        for file in os.listdir(source_data_path + \"regular/\" + species_id + \"/\"):\n",
    "            image_path  = source_data_path + \"regular/\" + species_id + \"/\" + file\n",
    "\n",
    "            savepath_common = \"/home/shivaram/DS/Projects/FishID/data/expanded_model_data/is_fish_common/train/\" + species_id + \"/\" + file\n",
    "            savepath_mixed = \"/home/shivaram/DS/Projects/FishID/data/expanded_model_data/is_fish_mixed/train/\" + species_id + \"/\" + file\n",
    "\n",
    "\n",
    "            os.symlink(image_path, savepath_common)\n",
    "            \n",
    "            if os.path.isfile(savepath_mixed):\n",
    "                skipped += 1\n",
    "            else:\n",
    "                os.symlink(image_path, savepath_mixed)\n",
    "                \n",
    "        print(f\"Skipped {skipped}/{fc} for {species_id} because it was already ported from the scientific dataset\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Expanded Dataset\n",
    "In this segment, I build the expanded dataset. The thresholds for this one are as follows: \n",
    "- images in fishbase: 10+\n",
    "- images in scientific or common: 30+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302\n"
     ]
    }
   ],
   "source": [
    "# Find the number of species with a minimum of 10+ images from fishbase\n",
    "thresh = 10\n",
    "mtt_species= []\n",
    "for species, count in sizes.items():\n",
    "    if count > thresh:\n",
    "        mtt_species.append(species)\n",
    "print(len(mtt_species))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286\n"
     ]
    }
   ],
   "source": [
    "# Find fish classes that have enough train data (50+ in scientific and common)\n",
    "all_thresh_ids = []\n",
    "int_ids = []\n",
    "train_thresh = 30\n",
    "for fid in mtt_species:\n",
    "    if common_counts[fid] > train_thresh and scientific_counts[fid] > train_thresh:\n",
    "        all_thresh_ids.append(fid)\n",
    "        int_ids.append(int(fid))\n",
    "print(len(all_thresh_ids))\n",
    "species_list = list(fish_df[fish_df[\"fishbase_id\"].isin(int_ids)][\"FishBase name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories needed for data\n",
    "if not os.path.isdir(\"/home/shivaram/DS/Projects/FishID/data/expanded_model_data/is_fish_scientific/\"):\n",
    "    for subfolder in [\"train/\", \"test/\", \"val/\"]:\n",
    "        for sid in all_thresh_ids:\n",
    "            os.makedirs(\"/home/shivaram/DS/Projects/FishID/data/expanded_model_data/is_fish_scientific/\" + subfolder + sid + \"/\")\n",
    "\n",
    "if not os.path.isdir(\"/home/shivaram/DS/Projects/FishID/data/expanded_model_data/is_fish_common/\"):\n",
    "    for subfolder in [\"train/\", \"test/\", \"val/\"]:\n",
    "        for sid in all_thresh_ids:\n",
    "            os.makedirs(\"/home/shivaram/DS/Projects/FishID/data/expanded_model_data/is_fish_common/\" + subfolder + sid + \"/\")\n",
    "\n",
    "\n",
    "if not os.path.isdir(\"/home/shivaram/DS/Projects/FishID/data/expanded_model_data/is_fish_mixed/\"):\n",
    "    for subfolder in [\"train/\", \"test/\", \"val/\"]:\n",
    "        for sid in all_thresh_ids:\n",
    "            os.makedirs(\"/home/shivaram/DS/Projects/FishID/data/expanded_model_data/is_fish_mixed/\" + subfolder + sid + \"/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 42/249 for 4355 because it was already ported from the scientific dataset\n",
      "Skipped 27/379 for 914 because it was already ported from the scientific dataset\n",
      "Skipped 139/369 for 5806 because it was already ported from the scientific dataset\n",
      "Skipped 23/435 for 5396 because it was already ported from the scientific dataset\n",
      "Skipped 111/354 for 6029 because it was already ported from the scientific dataset\n",
      "Skipped 54/261 for 10 because it was already ported from the scientific dataset\n",
      "Skipped 111/366 for 6024 because it was already ported from the scientific dataset\n",
      "Skipped 7/500 for 6468 because it was already ported from the scientific dataset\n",
      "Skipped 61/500 for 977 because it was already ported from the scientific dataset\n",
      "Skipped 89/330 for 6541 because it was already ported from the scientific dataset\n",
      "Skipped 10/90 for 84 because it was already ported from the scientific dataset\n",
      "Skipped 57/500 for 5983 because it was already ported from the scientific dataset\n",
      "Skipped 125/384 for 7293 because it was already ported from the scientific dataset\n",
      "Skipped 88/494 for 14300 because it was already ported from the scientific dataset\n",
      "Skipped 118/426 for 6630 because it was already ported from the scientific dataset\n",
      "Skipped 35/369 for 7814 because it was already ported from the scientific dataset\n",
      "Skipped 143/446 for 5838 because it was already ported from the scientific dataset\n",
      "Skipped 85/376 for 6032 because it was already ported from the scientific dataset\n",
      "Skipped 134/392 for 6033 because it was already ported from the scientific dataset\n",
      "Skipped 23/284 for 3615 because it was already ported from the scientific dataset\n",
      "Skipped 75/466 for 3089 because it was already ported from the scientific dataset\n",
      "Skipped 29/89 for 6396 because it was already ported from the scientific dataset\n",
      "Skipped 63/361 for 6380 because it was already ported from the scientific dataset\n",
      "Skipped 32/82 for 265 because it was already ported from the scientific dataset\n",
      "Skipped 99/355 for 5598 because it was already ported from the scientific dataset\n",
      "Skipped 27/92 for 3517 because it was already ported from the scientific dataset\n",
      "Skipped 57/209 for 333 because it was already ported from the scientific dataset\n",
      "Skipped 39/367 for 1041 because it was already ported from the scientific dataset\n",
      "Skipped 76/497 for 2467 because it was already ported from the scientific dataset\n",
      "Skipped 97/380 for 1260 because it was already ported from the scientific dataset\n",
      "Skipped 35/431 for 1072 because it was already ported from the scientific dataset\n",
      "Skipped 17/347 for 3385 because it was already ported from the scientific dataset\n",
      "Skipped 41/500 for 146 because it was already ported from the scientific dataset\n",
      "Skipped 48/379 for 4663 because it was already ported from the scientific dataset\n",
      "Skipped 58/442 for 5950 because it was already ported from the scientific dataset\n",
      "Skipped 53/363 for 1005 because it was already ported from the scientific dataset\n",
      "Skipped 105/371 for 4921 because it was already ported from the scientific dataset\n",
      "Skipped 63/363 for 5408 because it was already ported from the scientific dataset\n",
      "Skipped 19/213 for 877 because it was already ported from the scientific dataset\n",
      "Skipped 116/386 for 5352 because it was already ported from the scientific dataset\n",
      "Skipped 64/319 for 4274 because it was already ported from the scientific dataset\n",
      "Skipped 45/368 for 5584 because it was already ported from the scientific dataset\n",
      "Skipped 138/377 for 5952 because it was already ported from the scientific dataset\n",
      "Skipped 5/191 for 263 because it was already ported from the scientific dataset\n",
      "Skipped 125/476 for 6021 because it was already ported from the scientific dataset\n",
      "Skipped 31/204 for 2660 because it was already ported from the scientific dataset\n",
      "Skipped 10/378 for 239 because it was already ported from the scientific dataset\n",
      "Skipped 36/78 for 5611 because it was already ported from the scientific dataset\n",
      "Skipped 4/206 for 1007 because it was already ported from the scientific dataset\n",
      "Skipped 63/378 for 3 because it was already ported from the scientific dataset\n",
      "Skipped 111/385 for 1309 because it was already ported from the scientific dataset\n",
      "Skipped 66/276 for 5831 because it was already ported from the scientific dataset\n",
      "Skipped 43/188 for 966 because it was already ported from the scientific dataset\n",
      "Skipped 17/499 for 143 because it was already ported from the scientific dataset\n",
      "Skipped 114/364 for 4699 because it was already ported from the scientific dataset\n",
      "Skipped 7/356 for 142 because it was already ported from the scientific dataset\n",
      "Skipped 46/289 for 5989 because it was already ported from the scientific dataset\n",
      "Skipped 4/80 for 1266 because it was already ported from the scientific dataset\n",
      "Skipped 75/490 for 5389 because it was already ported from the scientific dataset\n",
      "Skipped 119/391 for 5564 because it was already ported from the scientific dataset\n",
      "Skipped 28/133 for 5512 because it was already ported from the scientific dataset\n",
      "Skipped 38/315 for 753 because it was already ported from the scientific dataset\n",
      "Skipped 91/379 for 5647 because it was already ported from the scientific dataset\n",
      "Skipped 37/350 for 1320 because it was already ported from the scientific dataset\n",
      "Skipped 17/396 for 217 because it was already ported from the scientific dataset\n",
      "Skipped 50/392 for 93 because it was already ported from the scientific dataset\n",
      "Skipped 11/86 for 1725 because it was already ported from the scientific dataset\n",
      "Skipped 112/369 for 4485 because it was already ported from the scientific dataset\n",
      "Skipped 92/366 for 2534 because it was already ported from the scientific dataset\n",
      "Skipped 66/330 for 8053 because it was already ported from the scientific dataset\n",
      "Skipped 7/375 for 28238 because it was already ported from the scientific dataset\n",
      "Skipped 57/394 for 5955 because it was already ported from the scientific dataset\n",
      "Skipped 80/378 for 1022 because it was already ported from the scientific dataset\n",
      "Skipped 48/391 for 1002 because it was already ported from the scientific dataset\n",
      "Skipped 61/348 for 6629 because it was already ported from the scientific dataset\n",
      "Skipped 30/115 for 1028 because it was already ported from the scientific dataset\n",
      "Skipped 110/293 for 7837 because it was already ported from the scientific dataset\n",
      "Skipped 122/416 for 4905 because it was already ported from the scientific dataset\n",
      "Skipped 16/56 for 5008 because it was already ported from the scientific dataset\n",
      "Skipped 159/500 for 5993 because it was already ported from the scientific dataset\n",
      "Skipped 104/379 for 5990 because it was already ported from the scientific dataset\n",
      "Skipped 5/97 for 1895 because it was already ported from the scientific dataset\n",
      "Skipped 30/172 for 81 because it was already ported from the scientific dataset\n",
      "Skipped 67/500 for 6556 because it was already ported from the scientific dataset\n",
      "Skipped 158/499 for 7874 because it was already ported from the scientific dataset\n",
      "Skipped 45/88 for 7187 because it was already ported from the scientific dataset\n",
      "Skipped 18/269 for 6018 because it was already ported from the scientific dataset\n",
      "Skipped 58/378 for 4786 because it was already ported from the scientific dataset\n",
      "Skipped 26/358 for 3232 because it was already ported from the scientific dataset\n",
      "Skipped 28/449 for 1751 because it was already ported from the scientific dataset\n",
      "Skipped 62/367 for 6604 because it was already ported from the scientific dataset\n",
      "Skipped 35/311 for 2389 because it was already ported from the scientific dataset\n",
      "Skipped 36/149 for 25509 because it was already ported from the scientific dataset\n",
      "Skipped 14/369 for 3382 because it was already ported from the scientific dataset\n",
      "Skipped 26/97 for 12302 because it was already ported from the scientific dataset\n",
      "Skipped 29/375 for 6578 because it was already ported from the scientific dataset\n",
      "Skipped 127/380 for 5833 because it was already ported from the scientific dataset\n",
      "Skipped 139/367 for 1265 because it was already ported from the scientific dataset\n",
      "Skipped 80/370 for 5579 because it was already ported from the scientific dataset\n",
      "Skipped 54/380 for 4659 because it was already ported from the scientific dataset\n",
      "Skipped 16/173 for 201 because it was already ported from the scientific dataset\n",
      "Skipped 34/377 for 4684 because it was already ported from the scientific dataset\n",
      "Skipped 79/370 for 4464 because it was already ported from the scientific dataset\n",
      "Skipped 33/356 for 998 because it was already ported from the scientific dataset\n",
      "Skipped 67/379 for 6662 because it was already ported from the scientific dataset\n",
      "Skipped 24/334 for 7641 because it was already ported from the scientific dataset\n",
      "Skipped 29/377 for 382 because it was already ported from the scientific dataset\n",
      "Skipped 27/80 for 5625 because it was already ported from the scientific dataset\n",
      "Skipped 65/328 for 5791 because it was already ported from the scientific dataset\n",
      "Skipped 69/394 for 2576 because it was already ported from the scientific dataset\n",
      "Skipped 69/355 for 5585 because it was already ported from the scientific dataset\n",
      "Skipped 166/492 for 5557 because it was already ported from the scientific dataset\n",
      "Skipped 20/272 for 1150 because it was already ported from the scientific dataset\n",
      "Skipped 107/360 for 5626 because it was already ported from the scientific dataset\n",
      "Skipped 40/373 for 875 because it was already ported from the scientific dataset\n",
      "Skipped 25/244 for 1312 because it was already ported from the scientific dataset\n",
      "Skipped 62/298 for 1951 because it was already ported from the scientific dataset\n",
      "Skipped 61/378 for 7 because it was already ported from the scientific dataset\n",
      "Skipped 69/378 for 868 because it was already ported from the scientific dataset\n",
      "Skipped 35/110 for 5390 because it was already ported from the scientific dataset\n",
      "Skipped 97/395 for 4276 because it was already ported from the scientific dataset\n",
      "Skipped 66/378 for 66478 because it was already ported from the scientific dataset\n",
      "Skipped 119/380 for 5444 because it was already ported from the scientific dataset\n",
      "Skipped 72/266 for 5758 because it was already ported from the scientific dataset\n",
      "Skipped 76/403 for 10206 because it was already ported from the scientific dataset\n",
      "Skipped 120/347 for 4534 because it was already ported from the scientific dataset\n",
      "Skipped 132/352 for 5824 because it was already ported from the scientific dataset\n",
      "Skipped 5/35 for 77 because it was already ported from the scientific dataset\n",
      "Skipped 54/350 for 114 because it was already ported from the scientific dataset\n",
      "Skipped 42/251 for 9950 because it was already ported from the scientific dataset\n",
      "Skipped 89/468 for 5580 because it was already ported from the scientific dataset\n",
      "Skipped 51/360 for 785 because it was already ported from the scientific dataset\n",
      "Skipped 45/270 for 1786 because it was already ported from the scientific dataset\n",
      "Skipped 78/369 for 6457 because it was already ported from the scientific dataset\n",
      "Skipped 44/348 for 1450 because it was already ported from the scientific dataset\n",
      "Skipped 78/401 for 5631 because it was already ported from the scientific dataset\n",
      "Skipped 7/374 for 6668 because it was already ported from the scientific dataset\n",
      "Skipped 5/84 for 880 because it was already ported from the scientific dataset\n",
      "Skipped 17/378 for 5446 because it was already ported from the scientific dataset\n",
      "Skipped 29/83 for 4296 because it was already ported from the scientific dataset\n",
      "Skipped 106/386 for 9259 because it was already ported from the scientific dataset\n",
      "Skipped 16/93 for 94 because it was already ported from the scientific dataset\n",
      "Skipped 93/349 for 5110 because it was already ported from the scientific dataset\n",
      "Skipped 80/358 for 4690 because it was already ported from the scientific dataset\n",
      "Skipped 20/197 for 1938 because it was already ported from the scientific dataset\n",
      "Skipped 66/380 for 7770 because it was already ported from the scientific dataset\n",
      "Skipped 27/277 for 12620 because it was already ported from the scientific dataset\n",
      "Skipped 97/374 for 10276 because it was already ported from the scientific dataset\n",
      "Skipped 15/436 for 6555 because it was already ported from the scientific dataset\n",
      "Skipped 47/372 for 5388 because it was already ported from the scientific dataset\n",
      "Skipped 3/90 for 290 because it was already ported from the scientific dataset\n",
      "Skipped 16/391 for 870 because it was already ported from the scientific dataset\n",
      "Skipped 17/500 for 4821 because it was already ported from the scientific dataset\n",
      "Skipped 59/378 for 5402 because it was already ported from the scientific dataset\n",
      "Skipped 22/180 for 6015 because it was already ported from the scientific dataset\n",
      "Skipped 0/365 for 89 because it was already ported from the scientific dataset\n",
      "Skipped 41/224 for 7812 because it was already ported from the scientific dataset\n",
      "Skipped 89/364 for 6030 because it was already ported from the scientific dataset\n",
      "Skipped 19/360 for 898 because it was already ported from the scientific dataset\n",
      "Skipped 96/378 for 5581 because it was already ported from the scientific dataset\n",
      "Skipped 82/394 for 5457 because it was already ported from the scientific dataset\n",
      "Skipped 112/444 for 5565 because it was already ported from the scientific dataset\n",
      "Skipped 90/371 for 1263 because it was already ported from the scientific dataset\n",
      "Skipped 38/491 for 874 because it was already ported from the scientific dataset\n",
      "Skipped 17/251 for 4908 because it was already ported from the scientific dataset\n",
      "Skipped 42/354 for 3231 because it was already ported from the scientific dataset\n",
      "Skipped 4/413 for 861 because it was already ported from the scientific dataset\n",
      "Skipped 85/371 for 1264 because it was already ported from the scientific dataset\n",
      "Skipped 38/80 for 6505 because it was already ported from the scientific dataset\n",
      "Skipped 117/368 for 5610 because it was already ported from the scientific dataset\n",
      "Skipped 34/379 for 1893 because it was already ported from the scientific dataset\n",
      "Skipped 81/298 for 6550 because it was already ported from the scientific dataset\n",
      "Skipped 30/377 for 5646 because it was already ported from the scientific dataset\n",
      "Skipped 52/85 for 1750 because it was already ported from the scientific dataset\n",
      "Skipped 82/295 for 387 because it was already ported from the scientific dataset\n",
      "Skipped 37/332 for 1044 because it was already ported from the scientific dataset\n",
      "Skipped 6/433 for 751 because it was already ported from the scientific dataset\n",
      "Skipped 34/337 for 4278 because it was already ported from the scientific dataset\n",
      "Skipped 138/376 for 7880 because it was already ported from the scientific dataset\n",
      "Skipped 24/80 for 4914 because it was already ported from the scientific dataset\n",
      "Skipped 61/394 for 859 because it was already ported from the scientific dataset\n",
      "Skipped 80/380 for 7251 because it was already ported from the scientific dataset\n",
      "Skipped 129/459 for 5606 because it was already ported from the scientific dataset\n",
      "Skipped 34/192 for 4984 because it was already ported from the scientific dataset\n",
      "Skipped 55/190 for 858 because it was already ported from the scientific dataset\n",
      "Skipped 128/395 for 6401 because it was already ported from the scientific dataset\n",
      "Skipped 34/207 for 1412 because it was already ported from the scientific dataset\n",
      "Skipped 77/379 for 1258 because it was already ported from the scientific dataset\n",
      "Skipped 102/393 for 5561 because it was already ported from the scientific dataset\n",
      "Skipped 89/349 for 6011 because it was already ported from the scientific dataset\n",
      "Skipped 48/318 for 14307 because it was already ported from the scientific dataset\n",
      "Skipped 71/489 for 5568 because it was already ported from the scientific dataset\n",
      "Skipped 10/206 for 2390 because it was already ported from the scientific dataset\n",
      "Skipped 49/366 for 993 because it was already ported from the scientific dataset\n",
      "Skipped 75/272 for 7872 because it was already ported from the scientific dataset\n",
      "Skipped 63/370 for 907 because it was already ported from the scientific dataset\n",
      "Skipped 77/348 for 6019 because it was already ported from the scientific dataset\n",
      "Skipped 33/321 for 4473 because it was already ported from the scientific dataset\n",
      "Skipped 14/86 for 4572 because it was already ported from the scientific dataset\n",
      "Skipped 12/38 for 1917 because it was already ported from the scientific dataset\n",
      "Skipped 42/193 for 5425 because it was already ported from the scientific dataset\n",
      "Skipped 69/202 for 4907 because it was already ported from the scientific dataset\n",
      "Skipped 96/376 for 5984 because it was already ported from the scientific dataset\n",
      "Skipped 32/400 for 107 because it was already ported from the scientific dataset\n",
      "Skipped 22/399 for 5562 because it was already ported from the scientific dataset\n",
      "Skipped 149/485 for 5835 because it was already ported from the scientific dataset\n",
      "Skipped 83/375 for 7777 because it was already ported from the scientific dataset\n",
      "Skipped 112/370 for 1261 because it was already ported from the scientific dataset\n",
      "Skipped 38/350 for 80 because it was already ported from the scientific dataset\n",
      "Skipped 91/322 for 5623 because it was already ported from the scientific dataset\n",
      "Skipped 108/352 for 5398 because it was already ported from the scientific dataset\n",
      "Skipped 113/360 for 1256 because it was already ported from the scientific dataset\n",
      "Skipped 10/184 for 262 because it was already ported from the scientific dataset\n",
      "Skipped 60/363 for 5734 because it was already ported from the scientific dataset\n",
      "Skipped 91/380 for 5891 because it was already ported from the scientific dataset\n",
      "Skipped 48/344 for 5596 because it was already ported from the scientific dataset\n",
      "Skipped 22/349 for 2061 because it was already ported from the scientific dataset\n",
      "Skipped 46/80 for 5796 because it was already ported from the scientific dataset\n",
      "Skipped 19/65 for 4915 because it was already ported from the scientific dataset\n",
      "Skipped 58/378 for 988 because it was already ported from the scientific dataset\n",
      "Skipped 29/123 for 1776 because it was already ported from the scientific dataset\n",
      "Skipped 123/381 for 1869 because it was already ported from the scientific dataset\n",
      "Skipped 100/456 for 1732 because it was already ported from the scientific dataset\n",
      "Skipped 13/363 for 7901 because it was already ported from the scientific dataset\n",
      "Skipped 56/379 for 412 because it was already ported from the scientific dataset\n",
      "Skipped 6/425 for 271 because it was already ported from the scientific dataset\n",
      "Skipped 43/101 for 7297 because it was already ported from the scientific dataset\n",
      "Skipped 37/368 for 3016 because it was already ported from the scientific dataset\n",
      "Skipped 20/356 for 116 because it was already ported from the scientific dataset\n",
      "Skipped 19/399 for 2081 because it was already ported from the scientific dataset\n",
      "Skipped 30/366 for 3226 because it was already ported from the scientific dataset\n",
      "Skipped 25/386 for 156 because it was already ported from the scientific dataset\n",
      "Skipped 18/474 for 1921 because it was already ported from the scientific dataset\n",
      "Skipped 9/265 for 886 because it was already ported from the scientific dataset\n",
      "Skipped 68/178 for 5586 because it was already ported from the scientific dataset\n",
      "Skipped 28/222 for 6395 because it was already ported from the scientific dataset\n",
      "Skipped 19/227 for 1390 because it was already ported from the scientific dataset\n",
      "Skipped 54/222 for 99 because it was already ported from the scientific dataset\n",
      "Skipped 10/84 for 7871 because it was already ported from the scientific dataset\n",
      "Skipped 22/154 for 3545 because it was already ported from the scientific dataset\n",
      "Skipped 28/88 for 4739 because it was already ported from the scientific dataset\n",
      "Skipped 32/201 for 4744 because it was already ported from the scientific dataset\n",
      "Skipped 24/324 for 23595 because it was already ported from the scientific dataset\n",
      "Skipped 10/352 for 226 because it was already ported from the scientific dataset\n",
      "Skipped 8/295 for 5804 because it was already ported from the scientific dataset\n",
      "Skipped 59/363 for 912 because it was already ported from the scientific dataset\n",
      "Skipped 97/480 for 4910 because it was already ported from the scientific dataset\n",
      "Skipped 137/495 for 6025 because it was already ported from the scientific dataset\n",
      "Skipped 27/80 for 5578 because it was already ported from the scientific dataset\n",
      "Skipped 69/363 for 5392 because it was already ported from the scientific dataset\n",
      "Skipped 117/378 for 5965 because it was already ported from the scientific dataset\n",
      "Skipped 62/479 for 65179 because it was already ported from the scientific dataset\n",
      "Skipped 19/244 for 4677 because it was already ported from the scientific dataset\n",
      "Skipped 8/315 for 3375 because it was already ported from the scientific dataset\n",
      "Skipped 163/372 for 4911 because it was already ported from the scientific dataset\n",
      "Skipped 6/355 for 7376 because it was already ported from the scientific dataset\n",
      "Skipped 55/411 for 5954 because it was already ported from the scientific dataset\n",
      "Skipped 77/401 for 637 because it was already ported from the scientific dataset\n",
      "Skipped 22/388 for 1077 because it was already ported from the scientific dataset\n",
      "Skipped 29/379 for 6504 because it was already ported from the scientific dataset\n",
      "Skipped 53/368 for 4676 because it was already ported from the scientific dataset\n",
      "Skipped 57/269 for 7306 because it was already ported from the scientific dataset\n",
      "Skipped 62/345 for 917 because it was already ported from the scientific dataset\n",
      "Skipped 17/374 for 5553 because it was already ported from the scientific dataset\n",
      "Skipped 18/184 for 4347 because it was already ported from the scientific dataset\n",
      "Skipped 52/360 for 3276 because it was already ported from the scientific dataset\n",
      "Skipped 51/409 for 343 because it was already ported from the scientific dataset\n",
      "Skipped 49/376 for 6 because it was already ported from the scientific dataset\n",
      "Skipped 25/64 for 1303 because it was already ported from the scientific dataset\n",
      "Skipped 2/352 for 457 because it was already ported from the scientific dataset\n",
      "Skipped 16/309 for 5836 because it was already ported from the scientific dataset\n",
      "Skipped 50/265 for 4284 because it was already ported from the scientific dataset\n",
      "Skipped 13/391 for 5805 because it was already ported from the scientific dataset\n",
      "Skipped 70/375 for 6635 because it was already ported from the scientific dataset\n",
      "Skipped 163/366 for 1235 because it was already ported from the scientific dataset\n",
      "Skipped 87/500 for 12600 because it was already ported from the scientific dataset\n",
      "Skipped 36/221 for 1906 because it was already ported from the scientific dataset\n",
      "Skipped 9/500 for 3228 because it was already ported from the scientific dataset\n",
      "Skipped 17/378 for 5923 because it was already ported from the scientific dataset\n",
      "Skipped 20/81 for 5689 because it was already ported from the scientific dataset\n",
      "Skipped 129/360 for 5645 because it was already ported from the scientific dataset\n",
      "Skipped 32/356 for 7778 because it was already ported from the scientific dataset\n",
      "Skipped 48/376 for 5403 because it was already ported from the scientific dataset\n",
      "Skipped 20/404 for 4275 because it was already ported from the scientific dataset\n",
      "Skipped 27/259 for 5839 because it was already ported from the scientific dataset\n",
      "Skipped 3/379 for 7813 because it was already ported from the scientific dataset\n"
     ]
    }
   ],
   "source": [
    "source_data_path =  \"/media/shivaram/SharedVolum/Projects/FishID/scraped_images/\"\n",
    "fishbase_source_path = \"/media/shivaram/SharedVolum/Projects/FishID/data/fishbase_images/\"\n",
    "\n",
    "save_dirs = [\"/home/shivaram/DS/Projects/FishID/data/expanded_model_data/is_fish_scientific/\", \"/home/shivaram/DS/Projects/FishID/data/expanded_model_data/is_fish_common/\", \"/home/shivaram/DS/Projects/FishID/data/expanded_model_data/is_fish_mixed/\"]\n",
    "\n",
    "random.seed(10)\n",
    "\n",
    "count = 0\n",
    "done = False \n",
    "for root_dir, cur_dir, files in os.walk(\"/home/shivaram/DS/Projects/FishID/data/expanded_model_data/is_fish_mixed/\"):\n",
    "    if len(files) > 0:\n",
    "        done = True\n",
    "\n",
    "        \n",
    "if not done:\n",
    "    for species_id in all_thresh_ids:\n",
    "\n",
    "        # Create validation and test sets from FishBase\n",
    "        for file in os.listdir(fishbase_source_path + species_id + \"/\"):\n",
    "            image_path = fishbase_source_path + species_id + \"/\" + file\n",
    "\n",
    "            # Create test and validation datasets (same for each of the different datasets)\n",
    "            test_val = bool(random.getrandbits(1))\n",
    "            if test_val: \n",
    "                save_folder = \"test/\"\n",
    "            else:\n",
    "                save_folder = \"val/\" \n",
    "            for save_dir in save_dirs:\n",
    "                save_path = save_dir + save_folder +  species_id + \"/\" + file \n",
    "                os.symlink(image_path, save_path)\n",
    "                \n",
    "        # Build scientific based dataset (and mixed)\n",
    "        for file in os.listdir(source_data_path + \"scientific/\" + species_id + \"/\"):\n",
    "            image_path  = source_data_path + \"scientific/\" + species_id + \"/\" + file\n",
    "\n",
    "            savepath_sci = \"/home/shivaram/DS/Projects/FishID/data/expanded_model_data/is_fish_scientific/train/\" + species_id + \"/\" + file\n",
    "            savepath_mixed = \"/home/shivaram/DS/Projects/FishID/data/expanded_model_data/is_fish_mixed/train/\" + species_id + \"/\" + file\n",
    "\n",
    "            os.symlink(image_path, savepath_sci)\n",
    "            os.symlink(image_path, savepath_mixed)\n",
    "\n",
    "\n",
    "        # Build common name dataset (and mixed)\n",
    "        skipped = 0\n",
    "        fc = len(os.listdir(source_data_path + \"regular/\" + species_id + \"/\"))\n",
    "        \n",
    "        for file in os.listdir(source_data_path + \"regular/\" + species_id + \"/\"):\n",
    "            image_path  = source_data_path + \"regular/\" + species_id + \"/\" + file\n",
    "\n",
    "            savepath_common = \"/home/shivaram/DS/Projects/FishID/data/expanded_model_data/is_fish_common/train/\" + species_id + \"/\" + file\n",
    "            savepath_mixed = \"/home/shivaram/DS/Projects/FishID/data/expanded_model_data/is_fish_mixed/train/\" + species_id + \"/\" + file\n",
    "\n",
    "\n",
    "            os.symlink(image_path, savepath_common)\n",
    "            \n",
    "            if os.path.isfile(savepath_mixed):\n",
    "                skipped += 1\n",
    "            else:\n",
    "                os.symlink(image_path, savepath_mixed)\n",
    "                \n",
    "        print(f\"Skipped {skipped}/{fc} for {species_id} because it was already ported from the scientific dataset\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
