{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as ds\n",
    "from torchvision import models, transforms, utils, datasets\n",
    "from torch.utils import data\n",
    "from torchsummary import summary\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in Is_Fish Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce RTX 3060 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "model_ft.load_state_dict(torch.load(\"models/is_fish.pt\"))\n",
    "model_ft.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idea for Workflow\n",
    "- Use Fishbase as a test and validation set\n",
    "- Build an initial proof of concept classifier for species that have enough data in FishBase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Fishbase Dataset\n",
    "### Determine Fishbase Test and Validation Sets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_df = pd.read_csv(\"data/fish_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = {}\n",
    "fishbase_source_path = \"data/fishbase_images/\"\n",
    "for species_dir in os.listdir(fishbase_source_path):\n",
    "    if species_dir == \".ipynb_checkpoints\":\n",
    "        continue\n",
    "    image_count = len([name for name in os.listdir(fishbase_source_path + species_dir + \"/\") if name.split(\".\")[-1] != \"gif\"])\n",
    "    #print(f\"{species_dir}: {image_count}\")\n",
    "    sizes[species_dir] = image_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(sizes.values())\n",
    "plt.title(\"File Count Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "# Find the number of species with a minimum count of images from fishbase\n",
    "thresh = 25\n",
    "mtt_species= []\n",
    "for species, count in sizes.items():\n",
    "    if count > thresh:\n",
    "        mtt_species.append(species)\n",
    "print(len(mtt_species))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Scraped Image Counts for the Species that Meet IsFish Threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([transforms.ToTensor(), transforms.Resize((224, 224)), ]) #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to parse a given directory and find the number of fish vs nonfish images\n",
    "def count_fish(species_path):\n",
    "    fish_count = 0\n",
    "    not_fish_count = 0\n",
    "    deltas = []\n",
    "    for file in os.listdir(species_path):\n",
    "        image_path = species_path + file\n",
    "        try:\n",
    "            img = Image.open(image_path).convert('RGB')\n",
    "        except:\n",
    "            continue\n",
    "        img_filt = data_transforms(img).cuda().unsqueeze(0)\n",
    "        outputs = model_ft(img_filt)\n",
    "\n",
    "        _, preds =torch.max(outputs, 1)\n",
    "        preds = preds.tolist()\n",
    "        fish_score = outputs.tolist()[0][0]\n",
    "        not_fish_score = outputs.tolist()[0][1]\n",
    "        delta = fish_score - not_fish_score \n",
    "        preds = preds[0]\n",
    "        # 1 is for not fish and 0 is for fish\n",
    "        mod_pred = delta > -2\n",
    "\n",
    "        if mod_pred == 1:\n",
    "            fish_count += 1\n",
    "\n",
    "        else:\n",
    "            not_fish_count += 1 \n",
    "            deltas.append(delta)\n",
    "            \n",
    "    return fish_count, not_fish_count, deltas\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "sci_path = \"/media/shivaram/SharedVolum/Projects/FishID/scraped_images/scientific/\" \n",
    "scientific_counts = dict()\n",
    "for species_id in mtt_species:\n",
    "    \n",
    "    try:\n",
    "        species_path = sci_path + species_id + \"/\"\n",
    "        ic, _, _ = count_fish(species_path)\n",
    "    except:\n",
    "        ic = 0\n",
    "    scientific_counts[species_id] = ic\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_path = \"/media/shivaram/SharedVolum/Projects/FishID/scraped_images/regular/\" \n",
    "common_counts = dict()\n",
    "for species_id in mtt_species:\n",
    "    try:\n",
    "        species_path = common_path + species_id + \"/\"\n",
    "        ic, _, _ = count_fish(species_path)\n",
    "    except:\n",
    "        ic = 0\n",
    "    common_counts[species_id] = ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    }
   ],
   "source": [
    "# Find fish classes that have enough train data\n",
    "all_thresh_ids = []\n",
    "train_thresh = 50\n",
    "for fid in mtt_species:\n",
    "    if common_counts[fid] > train_thresh and scientific_counts[fid] > train_thresh:\n",
    "        all_thresh_ids.append(fid)\n",
    "print(len(all_thresh_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
