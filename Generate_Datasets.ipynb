{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as ds\n",
    "from torchvision import models, transforms, utils, datasets\n",
    "from torch.utils import data\n",
    "from torchsummary import summary\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in Is_Fish Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce RTX 3060 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "model_ft.load_state_dict(torch.load(\"models/is_fish.pt\"))\n",
    "model_ft.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idea for Workflow\n",
    "- Use Fishbase as a test and validation set\n",
    "- Build an initial proof of concept classifier for species that have enough data in FishBase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Fishbase Dataset\n",
    "### Determine Fishbase Test and Validation Sets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_df = pd.read_csv(\"data/fish_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = {}\n",
    "fishbase_source_path = \"data/fishbase_images/\"\n",
    "for species_dir in os.listdir(fishbase_source_path):\n",
    "    if species_dir == \".ipynb_checkpoints\":\n",
    "        continue\n",
    "    image_count = len([name for name in os.listdir(fishbase_source_path + species_dir + \"/\") if name.split(\".\")[-1] != \"gif\"])\n",
    "    #print(f\"{species_dir}: {image_count}\")\n",
    "    sizes[species_dir] = image_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(sizes.values())\n",
    "plt.title(\"File Count Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "# Find the number of species with a minimum count of images from fishbase\n",
    "thresh = 25\n",
    "mtt_species= []\n",
    "for species, count in sizes.items():\n",
    "    if count > thresh:\n",
    "        mtt_species.append(species)\n",
    "print(len(mtt_species))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Scraped Image Counts for the Species that Meet IsFish Threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([transforms.ToTensor(), transforms.Resize((224, 224)), ]) #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to parse a given directory and find the number of fish vs nonfish images\n",
    "def count_fish(species_path):\n",
    "    fish_count = 0\n",
    "    not_fish_count = 0\n",
    "    deltas = []\n",
    "    image_paths = []\n",
    "    for file in os.listdir(species_path):\n",
    "        image_path = species_path + file\n",
    "        try:\n",
    "            img = Image.open(image_path).convert('RGB')\n",
    "        except:\n",
    "            continue\n",
    "        img_filt = data_transforms(img).cuda().unsqueeze(0)\n",
    "        outputs = model_ft(img_filt)\n",
    "\n",
    "        _, preds =torch.max(outputs, 1)\n",
    "        preds = preds.tolist()\n",
    "        fish_score = outputs.tolist()[0][0]\n",
    "        not_fish_score = outputs.tolist()[0][1]\n",
    "        delta = fish_score - not_fish_score \n",
    "        preds = preds[0]\n",
    "        # 1 is for not fish and 0 is for fish\n",
    "        mod_pred = delta > -2\n",
    "\n",
    "        if mod_pred == 1:\n",
    "            fish_count += 1\n",
    "            image_paths.append(image_path)\n",
    "\n",
    "        else:\n",
    "            not_fish_count += 1 \n",
    "            deltas.append(delta)\n",
    "            \n",
    "    return fish_count, not_fish_count, image_paths\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find scientific image count by species\n",
    "sci_path = \"/media/shivaram/SharedVolum/Projects/FishID/scraped_images/scientific/\" \n",
    "scientific_counts = dict()\n",
    "for species_id in mtt_species:\n",
    "    \n",
    "    try:\n",
    "        species_path = sci_path + species_id + \"/\"\n",
    "        ic, _, _ = count_fish(species_path)\n",
    "    except:\n",
    "        ic = 0\n",
    "    scientific_counts[species_id] = ic\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find common image count by species\n",
    "common_path = \"/media/shivaram/SharedVolum/Projects/FishID/scraped_images/regular/\" \n",
    "common_counts = dict()\n",
    "for species_id in mtt_species:\n",
    "    try:\n",
    "        species_path = common_path + species_id + \"/\"\n",
    "        ic, _, _ = count_fish(species_path)\n",
    "    except:\n",
    "        ic = 0\n",
    "    common_counts[species_id] = ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    }
   ],
   "source": [
    "# Find fish classes that have enough train data\n",
    "all_thresh_ids = []\n",
    "train_thresh = 50\n",
    "for fid in mtt_species:\n",
    "    if common_counts[fid] > train_thresh and scientific_counts[fid] > train_thresh:\n",
    "        all_thresh_ids.append(fid)\n",
    "print(len(all_thresh_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Symlink Directories for Train, Test, Val\n",
    "I am not sure how the model performance will change based on the training data, so I am trying to train it with three different training sets: once with scientific web scraped images, once with the common name web scraped images, and once with both combined. To do this however, I need to create training folder structures, which I do below via symlinks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories needed for data\n",
    "if not os.path.isdir(\"/home/shivaram/DS/Projects/FishID/data/model_data/is_fish_scientific/\"):\n",
    "    for subfolder in [\"train/\", \"test/\", \"val/\"]:\n",
    "        for sid in all_thresh_ids:\n",
    "            os.makedirs(\"/home/shivaram/DS/Projects/FishID/data/model_data/is_fish_scientific/\" + subfolder + sid + \"/\")\n",
    "\n",
    "if not os.path.isdir(\"/home/shivaram/DS/Projects/FishID/data/model_data/is_fish_common/\"):\n",
    "    for subfolder in [\"train/\", \"test/\", \"val/\"]:\n",
    "        for sid in all_thresh_ids:\n",
    "            os.makedirs(\"/home/shivaram/DS/Projects/FishID/data/model_data/is_fish_common/\" + subfolder + sid + \"/\")\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.isdir(\"/home/shivaram/DS/Projects/FishID/data/model_data/is_fish_mixed/\"):\n",
    "    for subfolder in [\"train/\", \"test/\", \"val/\"]:\n",
    "        for sid in all_thresh_ids:\n",
    "            os.makedirs(\"/home/shivaram/DS/Projects/FishID/data/model_data/is_fish_mixed/\" + subfolder + sid + \"/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 27/379 for 914 because it was already ported from the scientific dataset\n",
      "Skipped 111/366 for 6024 because it was already ported from the scientific dataset\n",
      "Skipped 7/500 for 6468 because it was already ported from the scientific dataset\n",
      "Skipped 61/500 for 977 because it was already ported from the scientific dataset\n",
      "Skipped 125/384 for 7293 because it was already ported from the scientific dataset\n",
      "Skipped 118/426 for 6630 because it was already ported from the scientific dataset\n",
      "Skipped 99/355 for 5598 because it was already ported from the scientific dataset\n",
      "Skipped 76/497 for 2467 because it was already ported from the scientific dataset\n",
      "Skipped 97/380 for 1260 because it was already ported from the scientific dataset\n",
      "Skipped 17/347 for 3385 because it was already ported from the scientific dataset\n",
      "Skipped 58/442 for 5950 because it was already ported from the scientific dataset\n",
      "Skipped 53/363 for 1005 because it was already ported from the scientific dataset\n",
      "Skipped 64/319 for 4274 because it was already ported from the scientific dataset\n",
      "Skipped 45/368 for 5584 because it was already ported from the scientific dataset\n",
      "Skipped 138/377 for 5952 because it was already ported from the scientific dataset\n",
      "Skipped 10/378 for 239 because it was already ported from the scientific dataset\n",
      "Skipped 4/206 for 1007 because it was already ported from the scientific dataset\n",
      "Skipped 63/378 for 3 because it was already ported from the scientific dataset\n",
      "Skipped 111/385 for 1309 because it was already ported from the scientific dataset\n",
      "Skipped 17/499 for 143 because it was already ported from the scientific dataset\n",
      "Skipped 38/315 for 753 because it was already ported from the scientific dataset\n",
      "Skipped 50/392 for 93 because it was already ported from the scientific dataset\n",
      "Skipped 112/369 for 4485 because it was already ported from the scientific dataset\n",
      "Skipped 92/366 for 2534 because it was already ported from the scientific dataset\n",
      "Skipped 80/378 for 1022 because it was already ported from the scientific dataset\n",
      "Skipped 48/391 for 1002 because it was already ported from the scientific dataset\n",
      "Skipped 122/416 for 4905 because it was already ported from the scientific dataset\n",
      "Skipped 5/97 for 1895 because it was already ported from the scientific dataset\n",
      "Skipped 67/500 for 6556 because it was already ported from the scientific dataset\n",
      "Skipped 139/367 for 1265 because it was already ported from the scientific dataset\n",
      "Skipped 54/380 for 4659 because it was already ported from the scientific dataset\n",
      "Skipped 34/377 for 4684 because it was already ported from the scientific dataset\n",
      "Skipped 33/356 for 998 because it was already ported from the scientific dataset\n",
      "Skipped 65/328 for 5791 because it was already ported from the scientific dataset\n",
      "Skipped 69/394 for 2576 because it was already ported from the scientific dataset\n",
      "Skipped 166/492 for 5557 because it was already ported from the scientific dataset\n",
      "Skipped 40/373 for 875 because it was already ported from the scientific dataset\n",
      "Skipped 69/378 for 868 because it was already ported from the scientific dataset\n",
      "Skipped 119/380 for 5444 because it was already ported from the scientific dataset\n",
      "Skipped 132/352 for 5824 because it was already ported from the scientific dataset\n",
      "Skipped 51/360 for 785 because it was already ported from the scientific dataset\n",
      "Skipped 44/348 for 1450 because it was already ported from the scientific dataset\n",
      "Skipped 78/401 for 5631 because it was already ported from the scientific dataset\n",
      "Skipped 97/374 for 10276 because it was already ported from the scientific dataset\n",
      "Skipped 15/436 for 6555 because it was already ported from the scientific dataset\n",
      "Skipped 0/365 for 89 because it was already ported from the scientific dataset\n",
      "Skipped 19/360 for 898 because it was already ported from the scientific dataset\n",
      "Skipped 38/491 for 874 because it was already ported from the scientific dataset\n",
      "Skipped 42/354 for 3231 because it was already ported from the scientific dataset\n",
      "Skipped 4/413 for 861 because it was already ported from the scientific dataset\n",
      "Skipped 52/85 for 1750 because it was already ported from the scientific dataset\n",
      "Skipped 82/295 for 387 because it was already ported from the scientific dataset\n",
      "Skipped 34/337 for 4278 because it was already ported from the scientific dataset\n",
      "Skipped 24/80 for 4914 because it was already ported from the scientific dataset\n",
      "Skipped 129/459 for 5606 because it was already ported from the scientific dataset\n",
      "Skipped 128/395 for 6401 because it was already ported from the scientific dataset\n",
      "Skipped 77/379 for 1258 because it was already ported from the scientific dataset\n",
      "Skipped 63/370 for 907 because it was already ported from the scientific dataset\n",
      "Skipped 42/193 for 5425 because it was already ported from the scientific dataset\n",
      "Skipped 32/400 for 107 because it was already ported from the scientific dataset\n",
      "Skipped 22/399 for 5562 because it was already ported from the scientific dataset\n",
      "Skipped 112/370 for 1261 because it was already ported from the scientific dataset\n",
      "Skipped 38/350 for 80 because it was already ported from the scientific dataset\n",
      "Skipped 91/322 for 5623 because it was already ported from the scientific dataset\n",
      "Skipped 108/352 for 5398 because it was already ported from the scientific dataset\n",
      "Skipped 91/380 for 5891 because it was already ported from the scientific dataset\n",
      "Skipped 48/344 for 5596 because it was already ported from the scientific dataset\n",
      "Skipped 58/378 for 988 because it was already ported from the scientific dataset\n",
      "Skipped 123/381 for 1869 because it was already ported from the scientific dataset\n",
      "Skipped 100/456 for 1732 because it was already ported from the scientific dataset\n",
      "Skipped 56/379 for 412 because it was already ported from the scientific dataset\n",
      "Skipped 6/425 for 271 because it was already ported from the scientific dataset\n",
      "Skipped 19/399 for 2081 because it was already ported from the scientific dataset\n",
      "Skipped 9/265 for 886 because it was already ported from the scientific dataset\n",
      "Skipped 10/352 for 226 because it was already ported from the scientific dataset\n",
      "Skipped 59/363 for 912 because it was already ported from the scientific dataset\n",
      "Skipped 137/495 for 6025 because it was already ported from the scientific dataset\n",
      "Skipped 22/388 for 1077 because it was already ported from the scientific dataset\n",
      "Skipped 29/379 for 6504 because it was already ported from the scientific dataset\n",
      "Skipped 62/345 for 917 because it was already ported from the scientific dataset\n",
      "Skipped 17/374 for 5553 because it was already ported from the scientific dataset\n",
      "Skipped 52/360 for 3276 because it was already ported from the scientific dataset\n",
      "Skipped 51/409 for 343 because it was already ported from the scientific dataset\n",
      "Skipped 49/376 for 6 because it was already ported from the scientific dataset\n",
      "Skipped 163/366 for 1235 because it was already ported from the scientific dataset\n",
      "Skipped 87/500 for 12600 because it was already ported from the scientific dataset\n",
      "Skipped 36/221 for 1906 because it was already ported from the scientific dataset\n",
      "Skipped 9/500 for 3228 because it was already ported from the scientific dataset\n",
      "Skipped 17/378 for 5923 because it was already ported from the scientific dataset\n",
      "Skipped 129/360 for 5645 because it was already ported from the scientific dataset\n",
      "Skipped 20/404 for 4275 because it was already ported from the scientific dataset\n",
      "Skipped 27/259 for 5839 because it was already ported from the scientific dataset\n"
     ]
    }
   ],
   "source": [
    "source_data_path =  \"/media/shivaram/SharedVolum/Projects/FishID/scraped_images/\"\n",
    "fishbase_source_path = \"/media/shivaram/SharedVolum/Projects/FishID/data/fishbase_images/\"\n",
    "\n",
    "save_dirs = [\"/home/shivaram/DS/Projects/FishID/data/model_data/is_fish_scientific/\", \"/home/shivaram/DS/Projects/FishID/data/model_data/is_fish_common/\", \"/home/shivaram/DS/Projects/FishID/data/model_data/is_fish_mixed/\"]\n",
    "\n",
    "random.seed(10)\n",
    "\n",
    "count = 0\n",
    "done = False \n",
    "for root_dir, cur_dir, files in os.walk(\"/home/shivaram/DS/Projects/FishID/data/model_data/is_fish_mixed/\"):\n",
    "    if len(files) > 0:\n",
    "        done = True\n",
    "\n",
    "        \n",
    "if not done:\n",
    "    for species_id in all_thresh_ids:\n",
    "\n",
    "        # Create validation and test sets from FishBase\n",
    "        for file in os.listdir(fishbase_source_path + species_id + \"/\"):\n",
    "            image_path = fishbase_source_path + species_id + \"/\" + file\n",
    "\n",
    "            # Create test and validation datasets (same for each of the different datasets)\n",
    "            test_val = bool(random.getrandbits(1))\n",
    "            if test_val: \n",
    "                save_folder = \"test/\"\n",
    "            else:\n",
    "                save_folder = \"val/\" \n",
    "            for save_dir in save_dirs:\n",
    "                save_path = save_dir + save_folder +  species_id + \"/\" + file \n",
    "                os.symlink(image_path, save_path)\n",
    "                \n",
    "        # Build scientific based dataset (and mixed)\n",
    "        for file in os.listdir(source_data_path + \"scientific/\" + species_id + \"/\"):\n",
    "            image_path  = source_data_path + \"scientific/\" + species_id + \"/\" + file\n",
    "\n",
    "            savepath_sci = \"/home/shivaram/DS/Projects/FishID/data/model_data/is_fish_scientific/train/\" + species_id + \"/\" + file\n",
    "            savepath_mixed = \"/home/shivaram/DS/Projects/FishID/data/model_data/is_fish_mixed/train/\" + species_id + \"/\" + file\n",
    "\n",
    "            os.symlink(image_path, savepath_sci)\n",
    "            os.symlink(image_path, savepath_mixed)\n",
    "\n",
    "\n",
    "        # Build common name dataset (and mixed)\n",
    "        skipped = 0\n",
    "        fc = len(os.listdir(source_data_path + \"regular/\" + species_id + \"/\"))\n",
    "        \n",
    "        for file in os.listdir(source_data_path + \"regular/\" + species_id + \"/\"):\n",
    "            image_path  = source_data_path + \"regular/\" + species_id + \"/\" + file\n",
    "\n",
    "            savepath_common = \"/home/shivaram/DS/Projects/FishID/data/model_data/is_fish_common/train/\" + species_id + \"/\" + file\n",
    "            savepath_mixed = \"/home/shivaram/DS/Projects/FishID/data/model_data/is_fish_mixed/train/\" + species_id + \"/\" + file\n",
    "\n",
    "\n",
    "            os.symlink(image_path, savepath_common)\n",
    "            if os.path.exists(savepath_mixed):\n",
    "                skipped += 1\n",
    "            else:\n",
    "                os.symlink(image_path, savepath_mixed)\n",
    "                # DELETING PRIORS WHY?????\n",
    "\n",
    "                \n",
    "        print(f\"Skipped {skipped}/{fc} for {species_id} because it was already ported from the scientific dataset\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['914',\n",
       " '6024',\n",
       " '6468',\n",
       " '977',\n",
       " '7293',\n",
       " '6630',\n",
       " '5598',\n",
       " '2467',\n",
       " '1260',\n",
       " '3385',\n",
       " '5950',\n",
       " '1005',\n",
       " '4274',\n",
       " '5584',\n",
       " '5952',\n",
       " '239',\n",
       " '1007',\n",
       " '3',\n",
       " '1309',\n",
       " '143',\n",
       " '753',\n",
       " '93',\n",
       " '4485',\n",
       " '2534',\n",
       " '1022',\n",
       " '1002',\n",
       " '4905',\n",
       " '1895',\n",
       " '6556',\n",
       " '1265',\n",
       " '4659',\n",
       " '4684',\n",
       " '998',\n",
       " '5791',\n",
       " '2576',\n",
       " '5557',\n",
       " '875',\n",
       " '868',\n",
       " '5444',\n",
       " '5824',\n",
       " '785',\n",
       " '1450',\n",
       " '5631',\n",
       " '10276',\n",
       " '6555',\n",
       " '89',\n",
       " '898',\n",
       " '874',\n",
       " '3231',\n",
       " '861',\n",
       " '1750',\n",
       " '387',\n",
       " '4278',\n",
       " '4914',\n",
       " '5606',\n",
       " '6401',\n",
       " '1258',\n",
       " '907',\n",
       " '5425',\n",
       " '107',\n",
       " '5562',\n",
       " '1261',\n",
       " '80',\n",
       " '5623',\n",
       " '5398',\n",
       " '5891',\n",
       " '5596',\n",
       " '988',\n",
       " '1869',\n",
       " '1732',\n",
       " '412',\n",
       " '271',\n",
       " '2081',\n",
       " '886',\n",
       " '226',\n",
       " '912',\n",
       " '6025',\n",
       " '1077',\n",
       " '6504',\n",
       " '917',\n",
       " '5553',\n",
       " '3276',\n",
       " '343',\n",
       " '6',\n",
       " '1235',\n",
       " '12600',\n",
       " '1906',\n",
       " '3228',\n",
       " '5923',\n",
       " '5645',\n",
       " '4275',\n",
       " '5839']"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_thresh_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/shivaram/DS/Projects/FishID/data/model_data/is_fish_mixed/train/5839/imagesqtbnand9gcs-extfg4rw9vkb7k0h5rgq36rp98utl7d8tqusqpcau.jpg'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "savepath_mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open(savepath_mixed)\n",
    "im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
