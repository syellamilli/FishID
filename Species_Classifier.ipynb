{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as ds\n",
    "from torchvision import models, transforms, utils, datasets\n",
    "from torch.utils import data\n",
    "from torchsummary import summary\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Read-In and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_df = pd.read_csv(\"data/fish_df.csv\")\n",
    "\n",
    "is_fish_df = pd.read_csv(\"data/is_fish.csv\").iloc[:,1:]\n",
    "is_fish_df[\"local_paths\"] = is_fish_df[\"Species\"].astype(str) + \"/\" + is_fish_df[\"Filename\"]\n",
    "path_set = set(is_fish_df[\"local_paths\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    acc_df = pd.read_csv(\"data/accuracies.csv\")\n",
    "except:\n",
    "    acc_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_count = 92 #len(is_fish_df[\"Species\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We normalize to imagenet mean for the data (https://stackoverflow.com/questions/58151507/why-pytorch-officially-use-mean-0-485-0-456-0-406-and-std-0-229-0-224-0-2)\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        transforms.Resize([224, 224])]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        transforms.Resize([224, 224])]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        transforms.Resize([224, 224])])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dirs = [\"/home/shivaram/DS/Projects/FishID/data/model_data/is_fish_scientific/\", \"/home/shivaram/DS/Projects/FishID/data/model_data/is_fish_common/\", \"/home/shivaram/DS/Projects/FishID/data/model_data/is_fish_mixed/\"]\n",
    "diff_image_datasets = {dd:{x: datasets.ImageFolder(os.path.join(dd, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val', 'test']} for dd in data_dirs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = diff_image_datasets[\"/home/shivaram/DS/Projects/FishID/data/model_data/is_fish_scientific/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64#8#32#64\n",
    "#epoch_samples = 2560# len(samples_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_samplers = {}\n",
    "for subset in [\"train\", \"val\", \"test\"]:\n",
    "    target = image_datasets[subset].targets\n",
    "    \n",
    "    if subset != \"test\":\n",
    "        class_sample_count =np.array([ len(np.where(target == t)[0]) for t in np.unique(target)])\n",
    "        weight = 1. / class_sample_count\n",
    "        samples_weight = np.array([weight[t] for t in target])\n",
    "        samples_weight = torch.from_numpy(samples_weight)\n",
    "        epoch_samples = len(samples_weight)\n",
    "\n",
    "        sampler = data.WeightedRandomSampler(samples_weight, epoch_samples)\n",
    "        weighted_samplers[subset] = sampler\n",
    "    else:\n",
    "        sampler = data.SequentialSampler(image_datasets[subset])\n",
    "        weighted_samplers[subset] = sampler\n",
    "    \n",
    "\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], \n",
    "                                              batch_size=batch_size, num_workers=4)\n",
    "              for x in ['train', 'val', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "class_names = image_datasets['train'].classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataloaders, model, criterion, optimizer, scheduler, num_epochs=25, verbose = True, plateau = False, early_stop = 15):\n",
    "    since = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    epoch_accs = []\n",
    "    \n",
    "    curr_val_acc = 0\n",
    "    stop_count = 0\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epa = {\"Epoch\": epoch}\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "            print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "            if not plateau and phase == 'train':\n",
    "                scheduler.step()\n",
    "            elif plateau and phase == 'val':\n",
    "                scheduler.step(epoch_loss)\n",
    "\n",
    "            \n",
    "            epa[f'{phase} Loss'] = float(epoch_loss)\n",
    "            epa[f'{phase} Accuracy'] = float(epoch_acc)\n",
    "            if verbose:\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "                \n",
    "            if phase == 'val':\n",
    "                if curr_val_acc < epoch_acc:\n",
    "                    curr_val_acc = epoch_acc\n",
    "                    stop_count = 0\n",
    "                else:\n",
    "                    stop_count += 1\n",
    "                \n",
    "\n",
    "        if verbose:\n",
    "            print()\n",
    "            \n",
    "        epoch_accs.append(epa)\n",
    "        \n",
    "        if stop_count >= early_stop:\n",
    "            print(f\"No Validation Accuracy decrease over the last {early_stop} epochs. Stopping training...\")\n",
    "            break\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    if verbose:\n",
    "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    epoch_accs = pd.DataFrame(epoch_accs)\n",
    "    return model, best_acc, epoch_accs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, criterion, phase = \"test\", verbose = True):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    ts = 0\n",
    "    \n",
    "    for inputs, labels in dataloaders[phase]:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        ts += len(preds)\n",
    "\n",
    "    epoch_loss = running_loss / ts\n",
    "    epoch_acc = running_corrects.double() / ts\n",
    "\n",
    "    if verbose:\n",
    "        print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        \n",
    "    return epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_save_model(save_path, dataloaders, model, criterion, optimizer, scheduler, num_epochs = 25, verbose = True, plateau = False, test = False, early_stop = 10):\n",
    "    \n",
    "    trained_model, val_acc, epoch_accs = train_model(dataloaders, model, criterion, optimizer, scheduler, num_epochs, verbose, plateau, early_stop = early_stop) \n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    if test:\n",
    "        test_acc = test_model(trained_model, criterion)\n",
    "    else:\n",
    "        test_acc = -1\n",
    "    if verbose:\n",
    "        print(f\"Val Accuracy: {val_acc}\")\n",
    "        print(f\"Test Accuracy: {test_acc}\")        \n",
    "\n",
    "    return trained_model, val_acc, test_acc, epoch_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_from_path(model_ft, path, image_datasets=image_datasets, weighted_samplers = weighted_samplers, decay = False, train_batch=64, test_batch=64, epochs = 30, early_stop = 10): #criterion = criterion, optimizer_ft = optimizer_ft, exp_lr_scheduler = exp_lr_scheduler, \n",
    "    if os.path.exists(path):\n",
    "        dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=test_batch, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "        model_ft.load_state_dict(torch.load(path))\n",
    "        test_model(model_ft, criterion)\n",
    "    else:\n",
    "        dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=train_batch, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "        if not decay:\n",
    "            model_ft, val_acc, test_acc, epoch_accs = train_test_save_model(path, dataloaders, model_ft, criterion, optimizer_ft, exp_lr_scheduler, epochs, True, early_stop = early_stop)\n",
    "        else:\n",
    "            model_ft, val_acc, test_acc, epoch_accs = train_test_save_model(path, dataloaders, model_ft, criterion, optimizer_ft, exp_lr_scheduler, epochs, True, True, early_stop = early_stop)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_k(model, criterion, phase = \"test\", verbose = True, k = 5):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    ts = 0\n",
    "    if k > 1:\n",
    "        print(f\"Evaluating Top {k} Accuracy...\")\n",
    "    \n",
    "    for inputs, labels in dataloaders[phase]:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        item_ind = 0\n",
    "        \n",
    "        for ind_preds in outputs:\n",
    "            item_class_rankings = sorted(range(92), key=lambda k: ind_preds[k], reverse = True)\n",
    "            correct_label = labels.data[item_ind]\n",
    "            rank = item_class_rankings.index(correct_label)\n",
    "            item_ind += 1\n",
    "            if rank < k:\n",
    "                running_corrects += 1\n",
    "            \n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        #running_corrects += torch.sum(preds == labels.data)\n",
    "        ts += len(preds)\n",
    "\n",
    "    epoch_loss = running_loss / ts\n",
    "    epoch_acc = running_corrects / ts\n",
    "\n",
    "    if verbose:\n",
    "        print(f'{k} Acc: {epoch_acc:.4f}')\n",
    "        \n",
    "    return epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce RTX 3060 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining the Best Transfer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Frozen Layers\n",
    "### ResNet18 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18_path = \"models/92_classifier/resnet18.pt\"\n",
    "\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "#for param in model_ft.parameters():\n",
    "#    param.requires_grad = False\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, species_count)\n",
    "model_ft = model_ft.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 2.0053 Acc: 0.6368\n"
     ]
    }
   ],
   "source": [
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "build_from_path(model_ft, resnet18_path, train_batch=128, test_batch=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decaying Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 2.2803 Acc: 0.6418\n"
     ]
    }
   ],
   "source": [
    "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 5)\n",
    "resnet18_path = \"models/92_classifier/resnet18_decay.pt\"\n",
    "build_from_path(model_ft, resnet18_path, train_batch=128, test_batch=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient Net B0\n",
    "Batch size for train 32 and batch size for eval 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnetb0_path = \"models/92_classifier/efficient_netb0.pt\"\n",
    "\n",
    "model_ft =  models.efficientnet_b0(pretrained = True)\n",
    "#for param in model_ft.parameters():\n",
    "#    param.requires_grad = False\n",
    "num_ftrs = model_ft.classifier[1].in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, species_count)\n",
    "model_ft = model_ft.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 1.6630 Acc: 0.6657\n"
     ]
    }
   ],
   "source": [
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "if os.path.exists(efficientnetb0_path):\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=8, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft.load_state_dict(torch.load(efficientnetb0_path))\n",
    "    test_model(model_ft, criterion)\n",
    "else:\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=32, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft, val_acc, test_acc, epoch_accs = train_test_save_model(efficientnetb0_path, dataloaders, model_ft, criterion, optimizer_ft, exp_lr_scheduler, 30, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decaying Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 2.0763 Acc: 0.6568\n"
     ]
    }
   ],
   "source": [
    "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 5)\n",
    "efficientnetb0_path = \"models/92_classifier/efficient_netb0_decay.pt\"\n",
    "if os.path.exists(efficientnetb0_path):\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=8, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft.load_state_dict(torch.load(efficientnetb0_path))\n",
    "    test_model(model_ft, criterion)\n",
    "else:\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=32, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft, val_acc, test_acc, epoch_accs = train_test_save_model(efficientnetb0_path, dataloaders, model_ft, criterion, optimizer_ft, exp_lr_scheduler, 30, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convnet Tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_path = \"models/92_classifier/conv_tiny.pt\"\n",
    "\n",
    "model_ft =  models.convnext_tiny(pretrained = True)\n",
    "#for param in model_ft.parameters():\n",
    "#    param.requires_grad = False\n",
    "num_ftrs = model_ft.classifier[2].in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, species_count)\n",
    "model_ft = model_ft.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 1.5263 Acc: 0.6886\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(conv_path):\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=8, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft.load_state_dict(torch.load(conv_path))\n",
    "    test_model(model_ft, criterion)\n",
    "else:\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=32, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft, val_acc, test_acc, epoch_accs = train_test_save_model(conv_path, dataloaders, model_ft, criterion, optimizer_ft, exp_lr_scheduler, 30, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decaying Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 7)\n",
    "conv_path = \"models/92_classifier/conv_tiny_decay.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 1.5263 Acc: 0.6886\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(conv_path):\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=8, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft.load_state_dict(torch.load(conv_path))\n",
    "    test_model(model_ft, criterion)\n",
    "else:\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=32, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft, val_acc, test_acc, epoch_accs = train_test_save_model(conv_path, dataloaders, model_ft, criterion, optimizer_ft, exp_lr_scheduler, 30, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All But Last Layer Frozen\n",
    "### ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18_path = \"models/92_classifier/rn18_frozen.pt\"\n",
    "\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, species_count)\n",
    "model_ft = model_ft.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.1, momentum=0.9)\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 15.0800 Acc: 0.4139\n"
     ]
    }
   ],
   "source": [
    "build_from_path(model_ft, resnet18_path, train_batch=32, test_batch=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient Net B0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnetb0_path = \"models/92_classifier/en_netb0_frozen_decay.pt\"\n",
    "\n",
    "model_ft =  models.efficientnet_b0(pretrained = True)\n",
    "\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "    \n",
    "num_ftrs = model_ft.classifier[1].in_features\n",
    "model_ft.classifier[1] = nn.Linear(num_ftrs, species_count)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.1, momentum=0.9)\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "#exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 2.7616 Acc: 0.4886\n"
     ]
    }
   ],
   "source": [
    "build_from_path(model_ft, efficientnetb0_path, train_batch=32, test_batch=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convnet Tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_path = \"models/92_classifier/conv_tiny_frozen.pt\"\n",
    "\n",
    "model_ft =  models.convnext_tiny(pretrained = True)\n",
    "\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "num_ftrs = model_ft.classifier[2].in_features\n",
    "\n",
    "# Greate Last Layer\n",
    "model_ft.classifier[2] = nn.Linear(num_ftrs, species_count)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.01, momentum=0.9)\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "#exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 1.6143 Acc: 0.5877\n"
     ]
    }
   ],
   "source": [
    "build_from_path(model_ft, conv_path, train_batch=32, test_batch=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Several Layers UnFrozen\n",
    "### Resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18_path = \"models/92_classifier/rn18_frozen_partial.pt\"\n",
    "\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model_ft.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, species_count)\n",
    "model_ft = model_ft.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.1, momentum=0.9)\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 2.1516 Acc: 0.6390\n"
     ]
    }
   ],
   "source": [
    "build_from_path(model_ft, resnet18_path, train_batch=128, test_batch=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnetb0_path = \"models/92_classifier/en_netb0_frozen_partial.pt\"\n",
    "\n",
    "model_ft =  models.efficientnet_b0(pretrained = True)\n",
    "\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model_ft.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_ft.avgpool.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_ft.features[7].parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "# Create Last Layer\n",
    "num_ftrs = model_ft.classifier[1].in_features\n",
    "model_ft.classifier[1] = nn.Linear(num_ftrs, species_count)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.1, momentum=0.9)\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 2.0592 Acc: 0.6039\n"
     ]
    }
   ],
   "source": [
    "build_from_path(model_ft, efficientnetb0_path, train_batch=8, test_batch=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convnet Tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft =  models.convnext_tiny(pretrained = True)\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model_ft.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_ft.avgpool.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_ft.features[7].parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "# Create Last Layer\n",
    "num_ftrs = model_ft.classifier[2].in_features\n",
    "model_ft.classifier[2] = nn.Linear(num_ftrs, species_count)\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 1.2974 Acc: 0.7086\n"
     ]
    }
   ],
   "source": [
    "conv_path = \"models/92_classifier/conv_tiny_frozen_partial.pt\"\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "build_from_path(model_ft, conv_path, train_batch=8, test_batch=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 1.2989 Acc: 0.7231\n"
     ]
    }
   ],
   "source": [
    "conv_path = \"models/92_classifier/conv_tiny_frozen_partial_decay.pt\"\n",
    "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 5)\n",
    "build_from_path(model_ft, conv_path, train_batch=8, test_batch=32, decay = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freezing Variations with Convnet Tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load already trained model\n",
    "model_ft =  models.convnext_tiny(pretrained = True)\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model_ft.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_ft.avgpool.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_ft.features[7].parameters():\n",
    "    param.requires_grad = True\n",
    "# Create Last Layer\n",
    "num_ftrs = model_ft.classifier[2].in_features\n",
    "model_ft.classifier[2] = nn.Linear(num_ftrs, species_count)\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "model_ft.load_state_dict(torch.load(\"models/92_classifier/conv_tiny_frozen_partial_decay.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6th Block+ Unfrozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unfreeze the next convolutional block\n",
    "for param in model_ft.features[6].parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 1.2989 Acc: 0.7231\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "conv_path = \"models/92_classifier/conv_tiny_frozen_partial2_decay.pt\"\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 5)\n",
    "build_from_path(model_ft, conv_path, train_batch=8, test_batch=32, decay = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5th Block+ Unfrozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load already trained model\n",
    "model_ft =  models.convnext_tiny(pretrained = True)\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model_ft.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_ft.avgpool.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_ft.features[7].parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_ft.features[6].parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_ft.features[5].parameters():\n",
    "    param.requires_grad = True\n",
    "# Create Last Layer\n",
    "num_ftrs = model_ft.classifier[2].in_features\n",
    "model_ft.classifier[2] = nn.Linear(num_ftrs, species_count)\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "model_ft.load_state_dict(torch.load(\"models/92_classifier/conv_tiny_frozen_partial2_decay.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 1.3248 Acc: 0.7231\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "conv_path = \"models/92_classifier/conv_tiny_frozen_partial3_decay.pt\"\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 5)\n",
    "build_from_path(model_ft, conv_path, train_batch=8, test_batch=32, decay = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6th Block+ Unfrozen from the Outset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load already trained model\n",
    "model_ft =  models.convnext_tiny(pretrained = True)\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model_ft.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_ft.avgpool.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_ft.features[7].parameters():\n",
    "    param.requires_grad = True\n",
    "#Unfreeze the next convolutional block\n",
    "for param in model_ft.features[6].parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "# Create Last Layer\n",
    "num_ftrs = model_ft.classifier[2].in_features\n",
    "model_ft.classifier[2] = nn.Linear(num_ftrs, species_count)\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 1.0332 Acc: 0.7738\n"
     ]
    }
   ],
   "source": [
    "conv_path = \"models/92_classifier/conv_tiny_frozen_partial2a_decay.pt\"\n",
    "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 5)\n",
    "build_from_path(model_ft, conv_path, train_batch=8, test_batch=32, decay = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combatting Overfitting\n",
    "### Ridge Regression\n",
    "https://stackoverflow.com/questions/42704283/adding-l1-l2-regularization-in-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WD = 0.0002\n",
      "test Loss: 0.9992 Acc: 0.7666\n",
      "WD = 7e-05\n",
      "test Loss: 1.1088 Acc: 0.7604\n"
     ]
    }
   ],
   "source": [
    "conv_path = \"models/92_classifier/ctf_p2ad_dropout.pt\"\n",
    "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 5)\n",
    "for wd in [0.0002, 0.00007]:#[0.0005, 0.0003, 0.0001, 0.00005, 0.00001]:\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"WD = {wd}\")\n",
    "    try:\n",
    "        conv_path_s = \"models/92_classifier/ctf_p2ad_dropout_\" + str(str(wd).split(\".\")[1] )+ \".pt\"\n",
    "    except:\n",
    "         conv_path_s = \"models/92_classifier/ctf_p2ad_dropout_\" + str(wd)+ \".pt\"\n",
    "    \n",
    "    # Load already trained model\n",
    "    model_ft =  models.convnext_tiny(pretrained = True)\n",
    "    for param in model_ft.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model_ft.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in model_ft.avgpool.parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in model_ft.features[7].parameters():\n",
    "        param.requires_grad = True\n",
    "    #Unfreeze the next convolutional block\n",
    "    for param in model_ft.features[6].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "\n",
    "\n",
    "    # Create Last Layer\n",
    "    num_ftrs = model_ft.classifier[2].in_features\n",
    "    model_ft.classifier[2] = nn.Linear(num_ftrs, species_count)\n",
    "    model_ft = model_ft.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.01, momentum=0.9, weight_decay=wd)\n",
    "    \n",
    "    build_from_path(model_ft, conv_path_s, train_batch=8, test_batch=32, decay = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Training Sets\n",
    "### Add Random Crops\n",
    "- https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0 \n",
    "- https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0/tables/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We normalize to imagenet mean for the data (https://stackoverflow.com/questions/58151507/why-pytorch-officially-use-mean-0-485-0-456-0-406-and-std-0-229-0-224-0-2)\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomResizedCrop(size=(224, 224)),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        transforms.Resize([224, 224])]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        transforms.Resize([224, 224])])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dirs = [\"/home/shivaram/DS/Projects/FishID/data/model_data/is_fish_scientific/\", \"/home/shivaram/DS/Projects/FishID/data/model_data/is_fish_common/\", \"/home/shivaram/DS/Projects/FishID/data/model_data/is_fish_mixed/\"]\n",
    "diff_image_datasets = {dd:{x: datasets.ImageFolder(os.path.join(dd, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val', 'test']} for dd in data_dirs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = diff_image_datasets[\"/home/shivaram/DS/Projects/FishID/data/model_data/is_fish_scientific/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_samplers = {}\n",
    "for subset in [\"train\", \"val\", \"test\"]:\n",
    "    target = image_datasets[subset].targets\n",
    "    \n",
    "    if True:#subset == \"train\":\n",
    "        class_sample_count =np.array([ len(np.where(target == t)[0]) for t in np.unique(target)])\n",
    "        weight = 1. / class_sample_count\n",
    "        samples_weight = np.array([weight[t] for t in target])\n",
    "        samples_weight = torch.from_numpy(samples_weight)\n",
    "        samples = 2048#len(samples_weight)\n",
    "        sampler = data.WeightedRandomSampler(samples_weight,samples)\n",
    "        weighted_samplers[subset] = sampler\n",
    "    else:\n",
    "        sampler = data.RandomSampler(image_datasets[subset])\n",
    "        weighted_samplers[subset] = sampler\n",
    "    \n",
    "\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], \n",
    "                                              batch_size=batch_size, num_workers=4)\n",
    "              for x in ['train', 'val', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "class_names = image_datasets['train'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load already trained model\n",
    "model_ft =  models.convnext_tiny(pretrained = True)\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model_ft.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_ft.avgpool.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_ft.features[7].parameters():\n",
    "    param.requires_grad = True\n",
    "#Unfreeze the next convolutional block\n",
    "for param in model_ft.features[6].parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "# Create Last Layer\n",
    "num_ftrs = model_ft.classifier[2].in_features\n",
    "model_ft.classifier[2] = nn.Linear(num_ftrs, species_count)\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.9069 Acc: 0.7939\n"
     ]
    }
   ],
   "source": [
    "conv_path = \"models/92_classifier/ct_fp2ad_random_crop.pt\"\n",
    "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 5)\n",
    "build_from_path(model_ft, conv_path, train_batch=8, test_batch=32, decay = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = diff_image_datasets[\"/home/shivaram/DS/Projects/FishID/data/model_data/is_fish_common/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_samplers = {}\n",
    "for subset in [\"train\", \"val\", \"test\"]:\n",
    "    target = image_datasets[subset].targets\n",
    "    \n",
    "    if True:#subset == \"train\":\n",
    "        class_sample_count =np.array([ len(np.where(target == t)[0]) for t in np.unique(target)])\n",
    "        weight = 1. / class_sample_count\n",
    "        samples_weight = np.array([weight[t] for t in target])\n",
    "        samples_weight = torch.from_numpy(samples_weight)\n",
    "        epoch_samples = len(samples_weight)\n",
    "        sampler = data.WeightedRandomSampler(samples_weight, epoch_samples)\n",
    "        weighted_samplers[subset] = sampler\n",
    "    else:\n",
    "        sampler = data.RandomSampler(image_datasets[subset])\n",
    "        weighted_samplers[subset] = sampler\n",
    "    \n",
    "\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], \n",
    "                                              batch_size=batch_size, num_workers=4)\n",
    "              for x in ['train', 'val', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "class_names = image_datasets['train'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load already trained model\n",
    "model_ft =  models.convnext_tiny(pretrained = True)\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model_ft.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_ft.avgpool.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_ft.features[7].parameters():\n",
    "    param.requires_grad = True\n",
    "#Unfreeze the next convolutional block\n",
    "for param in model_ft.features[6].parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "# Create Last Layer\n",
    "num_ftrs = model_ft.classifier[2].in_features\n",
    "model_ft.classifier[2] = nn.Linear(num_ftrs, species_count)\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.9146 Acc: 0.8033\n"
     ]
    }
   ],
   "source": [
    "conv_path = \"models/92_classifier/ct_fp2ad_common.pt\"\n",
    "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 5)\n",
    "build_from_path(model_ft, conv_path, train_batch=8, test_batch=32, decay = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = diff_image_datasets[\"/home/shivaram/DS/Projects/FishID/data/model_data/is_fish_mixed/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_samplers = {}\n",
    "for subset in [\"train\", \"val\", \"test\"]:\n",
    "    target = image_datasets[subset].targets\n",
    "    \n",
    "    if True:#subset == \"train\":\n",
    "        class_sample_count =np.array([ len(np.where(target == t)[0]) for t in np.unique(target)])\n",
    "        weight = 1. / class_sample_count\n",
    "        samples_weight = np.array([weight[t] for t in target])\n",
    "        samples_weight = torch.from_numpy(samples_weight)\n",
    "        epoch_samples = len(samples_weight)\n",
    "        sampler = data.WeightedRandomSampler(samples_weight, epoch_samples)\n",
    "        weighted_samplers[subset] = sampler\n",
    "    else:\n",
    "        sampler = data.RandomSampler(image_datasets[subset])\n",
    "        weighted_samplers[subset] = sampler\n",
    "    \n",
    "\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], \n",
    "                                              batch_size=batch_size, num_workers=4)\n",
    "              for x in ['train', 'val', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "class_names = image_datasets['train'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load already trained model\n",
    "model_ft =  models.convnext_tiny(pretrained = True)\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model_ft.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_ft.avgpool.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_ft.features[7].parameters():\n",
    "    param.requires_grad = True\n",
    "#Unfreeze the next convolutional block\n",
    "for param in model_ft.features[6].parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "# Create Last Layer\n",
    "num_ftrs = model_ft.classifier[2].in_features\n",
    "model_ft.classifier[2] = nn.Linear(num_ftrs, species_count)\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.8923 Acc: 0.8078\n"
     ]
    }
   ],
   "source": [
    "conv_path = \"models/92_classifier/ct_fp2ad_mixed.pt\"\n",
    "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 5)\n",
    "build_from_path(model_ft, conv_path, train_batch=8, test_batch=32, decay = True, epochs = 30, early_stop = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Mixed in Greater Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = diff_image_datasets[\"/home/shivaram/DS/Projects/FishID/data/model_data/is_fish_mixed/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples: 2048\n",
      "test Loss: 0.8919 Acc: 0.7969\n",
      "Samples: 15643\n",
      "test Loss: 1.0265 Acc: 0.7841\n"
     ]
    }
   ],
   "source": [
    "for samples in [2048, 15643]:#, 31285, 46928, 62571]:\n",
    "    print(f\"Samples: {samples}\")\n",
    "    # Build dataloaders\n",
    "    weighted_samplers = {}\n",
    "    for subset in [\"train\", \"val\", \"test\"]:\n",
    "        target = image_datasets[subset].targets\n",
    "\n",
    "        if True:#subset == \"train\":\n",
    "            class_sample_count =np.array([ len(np.where(target == t)[0]) for t in np.unique(target)])\n",
    "            weight = 1. / class_sample_count\n",
    "            samples_weight = np.array([weight[t] for t in target])\n",
    "            samples_weight = torch.from_numpy(samples_weight)\n",
    "\n",
    "            sampler = data.WeightedRandomSampler(samples_weight,samples)\n",
    "            weighted_samplers[subset] = sampler\n",
    "        else:\n",
    "            sampler = data.RandomSampler(image_datasets[subset])\n",
    "            weighted_samplers[subset] = sampler\n",
    "\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], \n",
    "                                                  batch_size=batch_size, num_workers=4)\n",
    "                  for x in ['train', 'val', 'test']}\n",
    "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "    class_names = image_datasets['train'].classes\n",
    "    \n",
    "    # Load already trained model\n",
    "    model_ft =  models.convnext_tiny(pretrained = True)\n",
    "    for param in model_ft.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model_ft.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in model_ft.avgpool.parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in model_ft.features[7].parameters():\n",
    "        param.requires_grad = True\n",
    "    #Unfreeze the next convolutional block\n",
    "    for param in model_ft.features[6].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Create Last Layer\n",
    "    num_ftrs = model_ft.classifier[2].in_features\n",
    "    model_ft.classifier[2] = nn.Linear(num_ftrs, species_count)\n",
    "    model_ft = model_ft.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.01, momentum=0.9)\n",
    "    \n",
    "    conv_path = f'models/92_classifier/ct_fp2ad_mixed_{samples}.pt'\n",
    "    exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 5)\n",
    "    build_from_path(model_ft, conv_path, train_batch=8, test_batch=32, decay = True, epochs = 30, early_stop = 15)\n",
    "    #build_from_path(model_ft, conv_path, train_batch=8, test_batch=32, decay = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning Models with Additional Data\n",
    "### Scientific Fine Tuned on Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We normalize to imagenet mean for the data (https://stackoverflow.com/questions/58151507/why-pytorch-officially-use-mean-0-485-0-456-0-406-and-std-0-229-0-224-0-2)\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomResizedCrop(size=(224, 224)),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        transforms.Resize([224, 224])]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        transforms.Resize([224, 224])])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = diff_image_datasets[\"/home/shivaram/DS/Projects/FishID/data/model_data/is_fish_common/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load already trained model\n",
    "model_ft =  models.convnext_tiny(pretrained = True)\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model_ft.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_ft.avgpool.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_ft.features[7].parameters():\n",
    "    param.requires_grad = True\n",
    "#Unfreeze the next convolutional block\n",
    "for param in model_ft.features[6].parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "# Create Last Layer\n",
    "num_ftrs = model_ft.classifier[2].in_features\n",
    "model_ft.classifier[2] = nn.Linear(num_ftrs, species_count)\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.9645 Acc: 0.7881\n"
     ]
    }
   ],
   "source": [
    "source_path = \"models/92_classifier/ct_fp2ad_random_crop.pt\"\n",
    "model_ft.load_state_dict(torch.load(source_path))\n",
    "\n",
    "conv_path = \"models/92_classifier/ct_fp2adrc_sci_common.pt\"\n",
    "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 5)\n",
    "build_from_path(model_ft, conv_path, train_batch=8, test_batch=32, decay = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scientific Fine Tuned on Mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.9956 Acc: 0.7880\n"
     ]
    }
   ],
   "source": [
    "image_datasets = diff_image_datasets[\"/home/shivaram/DS/Projects/FishID/data/model_data/is_fish_mixed/\"]\n",
    "model_ft.load_state_dict(torch.load(source_path))\n",
    "\n",
    "conv_path = \"models/92_classifier/ct_fp2adrc_sci_mixed.pt\"\n",
    "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 5)\n",
    "build_from_path(model_ft, conv_path, train_batch=8, test_batch=32, decay = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing with Smaller Initial Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 1.0051 Acc: 0.7825\n"
     ]
    }
   ],
   "source": [
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.0001, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 5)\n",
    "model_ft.load_state_dict(torch.load(source_path))\n",
    "\n",
    "conv_path = \"models/92_classifier/ct_fp2adrc_sci_mixed_0001.pt\"\n",
    "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 5)\n",
    "build_from_path(model_ft, conv_path, train_batch=8, test_batch=32, decay = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common on Mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.9684 Acc: 0.7928\n"
     ]
    }
   ],
   "source": [
    "source_path = \"models/92_classifier/ct_fp2ad_common.pt\"\n",
    "model_ft.load_state_dict(torch.load(source_path))\n",
    "\n",
    "conv_path = \"models/92_classifier/ct_fp2adrc_common_mixed.pt\"\n",
    "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 5)\n",
    "build_from_path(model_ft, conv_path, train_batch=8, test_batch=32, decay = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top N Accuracy of Best Model \n",
    "### (Partially Unfrozen ConvNet Tiny on Mixed Data, with Reduce LR on Plateau, and Random Crops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_path = \"models/92_classifier/ct_fp2ad_mixed.pt\"\n",
    "model_ft.load_state_dict(torch.load(best_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Top 3 Accuracy...\n",
      "3 Acc: 0.9003\n",
      "Evaluating Top 5 Accuracy...\n",
      "5 Acc: 0.9287\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9286908077994429"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "test_model_k(model_ft, criterion, k=3)\n",
    "test_model_k(model_ft, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
