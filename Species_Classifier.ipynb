{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as ds\n",
    "from torchvision import models, transforms, utils, datasets\n",
    "from torch.utils import data\n",
    "from torchsummary import summary\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Read-In and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_df = pd.read_csv(\"data/fish_df.csv\")\n",
    "\n",
    "is_fish_df = pd.read_csv(\"data/is_fish.csv\").iloc[:,1:]\n",
    "is_fish_df[\"local_paths\"] = is_fish_df[\"Species\"].astype(str) + \"/\" + is_fish_df[\"Filename\"]\n",
    "path_set = set(is_fish_df[\"local_paths\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    acc_df = pd.read_csv(\"data/accuracies.csv\")\n",
    "except:\n",
    "    acc_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_count = 92 #len(is_fish_df[\"Species\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We normalize to imagenet mean for the data (https://stackoverflow.com/questions/58151507/why-pytorch-officially-use-mean-0-485-0-456-0-406-and-std-0-229-0-224-0-2)\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        transforms.Resize([224, 224])]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        transforms.Resize([224, 224])]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        transforms.Resize([224, 224])])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dirs = [\"/home/shivaram/DS/Projects/FishID/data/model_data/is_fish_scientific/\", \"/home/shivaram/DS/Projects/FishID/data/model_data/is_fish_common/\", \"/home/shivaram/DS/Projects/FishID/data/model_data/is_fish_mixed/\"]\n",
    "diff_image_datasets = {dd:{x: datasets.ImageFolder(os.path.join(dd, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val', 'test']} for dd in data_dirs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = diff_image_datasets[\"/home/shivaram/DS/Projects/FishID/data/model_data/is_fish_scientific/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64#8#32#64\n",
    "#epoch_samples = 2560# len(samples_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_samplers = {}\n",
    "for subset in [\"train\", \"val\", \"test\"]:\n",
    "    target = image_datasets[subset].targets\n",
    "    \n",
    "    if True:#subset == \"train\":\n",
    "        class_sample_count =np.array([ len(np.where(target == t)[0]) for t in np.unique(target)])\n",
    "        weight = 1. / class_sample_count\n",
    "        samples_weight = np.array([weight[t] for t in target])\n",
    "        samples_weight = torch.from_numpy(samples_weight)\n",
    "        epoch_samples = len(samples_weight)\n",
    "        sampler = data.WeightedRandomSampler(samples_weight, epoch_samples)\n",
    "        weighted_samplers[subset] = sampler\n",
    "    else:\n",
    "        sampler = data.RandomSampler(image_datasets[subset])\n",
    "        weighted_samplers[subset] = sampler\n",
    "    \n",
    "\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], \n",
    "                                              batch_size=batch_size, num_workers=4)\n",
    "              for x in ['train', 'val', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "class_names = image_datasets['train'].classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataloaders, model, criterion, optimizer, scheduler, num_epochs=25, verbose = True, plateau = False, early_stop = 15):\n",
    "    since = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    epoch_accs = []\n",
    "    \n",
    "    curr_val_acc = 0\n",
    "    stop_count = 0\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epa = {\"Epoch\": epoch}\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "            print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "            if not plateau and phase == 'train':\n",
    "                scheduler.step()\n",
    "            elif plateau and phase == 'val':\n",
    "                scheduler.step(epoch_loss)\n",
    "\n",
    "            \n",
    "            epa[f'{phase} Loss'] = float(epoch_loss)\n",
    "            epa[f'{phase} Accuracy'] = float(epoch_acc)\n",
    "            if verbose:\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "                \n",
    "            if phase == 'val':\n",
    "                if curr_val_acc < epoch_acc:\n",
    "                    curr_val_acc = epoch_acc\n",
    "                    stop_count = 0\n",
    "                else:\n",
    "                    stop_count += 1\n",
    "                \n",
    "\n",
    "        if verbose:\n",
    "            print()\n",
    "            \n",
    "        epoch_accs.append(epa)\n",
    "        \n",
    "        if stop_count >= early_stop:\n",
    "            print(f\"No Validation Accuracy decrease over the last {early_stop} epochs. Stopping training...\")\n",
    "            break\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    if verbose:\n",
    "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    epoch_accs = pd.DataFrame(epoch_accs)\n",
    "    return model, best_acc, epoch_accs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, criterion, phase = \"test\", verbose = True):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    for inputs, labels in dataloaders[phase]:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    epoch_loss = running_loss / dataset_sizes[phase]\n",
    "    epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "    if verbose:\n",
    "        print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        \n",
    "    return epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_save_model(save_path, dataloaders, model, criterion, optimizer, scheduler, num_epochs = 25, verbose = True, plateau = False, test = False, early_stop = 10):\n",
    "    \n",
    "    trained_model, val_acc, epoch_accs = train_model(dataloaders, model, criterion, optimizer, scheduler, num_epochs, verbose, plateau, early_stop = early_stop) \n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    if test:\n",
    "        test_acc = test_model(trained_model, criterion)\n",
    "    else:\n",
    "        test_acc = -1\n",
    "    if verbose:\n",
    "        print(f\"Val Accuracy: {val_acc}\")\n",
    "        print(f\"Test Accuracy: {test_acc}\")        \n",
    "\n",
    "    return trained_model, val_acc, test_acc, epoch_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_from_path(model_ft, path, image_datasets=image_datasets, weighted_samplers = weighted_samplers, decay = False, train_batch=64, test_batch=64, epochs = 30, early_stop = 10): #criterion = criterion, optimizer_ft = optimizer_ft, exp_lr_scheduler = exp_lr_scheduler, \n",
    "    if os.path.exists(path):\n",
    "        dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=train_batch, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "        model_ft.load_state_dict(torch.load(path))\n",
    "        test_model(model_ft, criterion)\n",
    "    else:\n",
    "        dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=test_batch, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "        if not decay:\n",
    "            model_ft, val_acc, test_acc, epoch_accs = train_test_save_model(path, dataloaders, model_ft, criterion, optimizer_ft, exp_lr_scheduler, epochs, True, early_stop = early_stop)\n",
    "        else:\n",
    "            model_ft, val_acc, test_acc, epoch_accs = train_test_save_model(path, dataloaders, model_ft, criterion, optimizer_ft, exp_lr_scheduler, epochs, True, True, early_stop = early_stop)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining the Best Transfer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce RTX 3060 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Frozen Layers\n",
    "### ResNet18 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18_path = \"models/92_classifier/resnet18.pt\"\n",
    "\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "#for param in model_ft.parameters():\n",
    "#    param.requires_grad = False\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, species_count)\n",
    "model_ft = model_ft.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 1.8165 Acc: 0.6646\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(resnet18_path):\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=64, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft.load_state_dict(torch.load(resnet18_path))\n",
    "    test_model(model_ft, criterion)\n",
    "else:\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=128, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft, val_acc, test_acc, epoch_accs = train_test_save_model(resnet18_path, dataloaders, model_ft, criterion, optimizer_ft, exp_lr_scheduler, 30, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decaying Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 2.0868 Acc: 0.6691\n"
     ]
    }
   ],
   "source": [
    "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 5)\n",
    "resnet18_path = \"models/92_classifier/resnet18_decay.pt\"\n",
    "\n",
    "if os.path.exists(resnet18_path):\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=64, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft.load_state_dict(torch.load(resnet18_path))\n",
    "    test_model(model_ft, criterion)\n",
    "else:\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=128, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft, val_acc, test_acc, epoch_accs = train_test_save_model(resnet18_path, dataloaders, model_ft, criterion, optimizer_ft, exp_lr_scheduler, 30, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient Net B0\n",
    "Batch size for train 32 and batch size for eval 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnetb0_path = \"models/92_classifier/efficient_netb0.pt\"\n",
    "\n",
    "model_ft =  models.efficientnet_b0(pretrained = True)\n",
    "#for param in model_ft.parameters():\n",
    "#    param.requires_grad = False\n",
    "num_ftrs = model_ft.classifier[1].in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, species_count)\n",
    "model_ft = model_ft.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m     dataloaders \u001b[38;5;241m=\u001b[39m {x: torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(image_datasets[x], sampler \u001b[38;5;241m=\u001b[39m weighted_samplers[x], batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[0;32m----> 9\u001b[0m     model_ft, val_acc, test_acc, epoch_accs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_save_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mefficientnetb0_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_ft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_ft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_lr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mtrain_test_save_model\u001b[0;34m(save_path, dataloaders, model, criterion, optimizer, scheduler, num_epochs, verbose, plateau)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_test_save_model\u001b[39m(save_path, dataloaders, model, criterion, optimizer, scheduler, num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m25\u001b[39m, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, plateau \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m----> 3\u001b[0m     trained_model, val_acc, epoch_accs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplateau\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      4\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), save_path)\n\u001b[1;32m      5\u001b[0m     test_acc \u001b[38;5;241m=\u001b[39m test_model(trained_model, criterion)\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(dataloaders, model, criterion, optimizer, scheduler, num_epochs, verbose, plateau)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m phase \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     43\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 44\u001b[0m         \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# statistics\u001b[39;00m\n\u001b[1;32m     47\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:65\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     64\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/sgd.py:144\u001b[0m, in \u001b[0;36mSGD.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m             momentum_buffer_list\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmomentum_buffer\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 144\u001b[0m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msgd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m      \u001b[49m\u001b[43md_p_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m      \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m      \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdampening\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdampening\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnesterov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# update momentum_buffers in state\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p, momentum_buffer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(params_with_grad, momentum_buffer_list):\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/optim/_functional.py:186\u001b[0m, in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[1;32m    184\u001b[0m     momentum_buffer_list[i] \u001b[38;5;241m=\u001b[39m buf\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     \u001b[43mbuf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdampening\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nesterov:\n\u001b[1;32m    189\u001b[0m     d_p \u001b[38;5;241m=\u001b[39m d_p\u001b[38;5;241m.\u001b[39madd(buf, alpha\u001b[38;5;241m=\u001b[39mmomentum)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "if os.path.exists(efficientnetb0_path):\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=8, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft.load_state_dict(torch.load(efficientnetb0_path))\n",
    "    test_model(model_ft, criterion)\n",
    "else:\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=32, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft, val_acc, test_acc, epoch_accs = train_test_save_model(efficientnetb0_path, dataloaders, model_ft, criterion, optimizer_ft, exp_lr_scheduler, 30, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decaying Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 2.0512 Acc: 0.6808\n"
     ]
    }
   ],
   "source": [
    "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 5)\n",
    "efficientnetb0_path = \"models/92_classifier/efficient_netb0_decay.pt\"\n",
    "if os.path.exists(efficientnetb0_path):\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=8, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft.load_state_dict(torch.load(efficientnetb0_path))\n",
    "    test_model(model_ft, criterion)\n",
    "else:\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=32, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft, val_acc, test_acc, epoch_accs = train_test_save_model(efficientnetb0_path, dataloaders, model_ft, criterion, optimizer_ft, exp_lr_scheduler, 30, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convnet Tiny\n",
    "Batch size for train 32 and for test 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_path = \"models/92_classifier/conv_tiny.pt\"\n",
    "\n",
    "model_ft =  models.convnext_tiny(pretrained = True)\n",
    "#for param in model_ft.parameters():\n",
    "#    param.requires_grad = False\n",
    "num_ftrs = model_ft.classifier[2].in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, species_count)\n",
    "model_ft = model_ft.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 1.3758 Acc: 0.7081\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "#exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 3)\n",
    "\n",
    "if os.path.exists(conv_path):\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=8, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft.load_state_dict(torch.load(conv_path))\n",
    "    test_model(model_ft, criterion)\n",
    "else:\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=32, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft, val_acc, test_acc, epoch_accs = train_test_save_model(conv_path, dataloaders, model_ft, criterion, optimizer_ft, exp_lr_scheduler, 30, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decaying Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 1.7669 Acc: 0.7359\n"
     ]
    }
   ],
   "source": [
    "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 7)\n",
    "conv_path = \"models/92_classifier/conv_tiny_decay.pt\"\n",
    "if os.path.exists(conv_path):\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=8, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft.load_state_dict(torch.load(conv_path))\n",
    "    test_model(model_ft, criterion)\n",
    "else:\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=32, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft, val_acc, test_acc, epoch_accs = train_test_save_model(conv_path, dataloaders, model_ft, criterion, optimizer_ft, exp_lr_scheduler, 30, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All But Last Layer Frozen\n",
    "### ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18_path = \"models/92_classifier/rn18_frozen.pt\"\n",
    "\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, species_count)\n",
    "model_ft = model_ft.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.1, momentum=0.9)\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "#exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient Net B0\n",
    "\n",
    "https://discuss.pytorch.org/t/how-the-pytorch-freeze-network-in-some-layers-only-the-rest-of-the-training/7088/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "----------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "step() missing 1 required positional argument: 'metrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     dataloaders \u001b[38;5;241m=\u001b[39m {x: torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(image_datasets[x], sampler \u001b[38;5;241m=\u001b[39m weighted_samplers[x], batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[0;32m---> 25\u001b[0m     model_ft, val_acc, test_acc, epoch_accs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_save_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mefficientnetb0_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_ft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_ft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_lr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mtrain_test_save_model\u001b[0;34m(save_path, dataloaders, model, criterion, optimizer, scheduler, num_epochs, verbose, plateau, test)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_test_save_model\u001b[39m(save_path, dataloaders, model, criterion, optimizer, scheduler, num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m25\u001b[39m, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, plateau \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m----> 3\u001b[0m     trained_model, val_acc, epoch_accs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplateau\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      4\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), save_path)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m test:\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(dataloaders, model, criterion, optimizer, scheduler, num_epochs, verbose, plateau)\u001b[0m\n\u001b[1;32m     51\u001b[0m epoch_acc \u001b[38;5;241m=\u001b[39m running_corrects\u001b[38;5;241m.\u001b[39mdouble() \u001b[38;5;241m/\u001b[39m dataset_sizes[phase]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m plateau \u001b[38;5;129;01mand\u001b[39;00m phase \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m plateau \u001b[38;5;129;01mand\u001b[39;00m phase \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     56\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep(epoch_loss)\n",
      "\u001b[0;31mTypeError\u001b[0m: step() missing 1 required positional argument: 'metrics'"
     ]
    }
   ],
   "source": [
    "efficientnetb0_path = \"models/92_classifier/en_netb0_frozen_decay.pt\"\n",
    "\n",
    "model_ft =  models.efficientnet_b0(pretrained = True)\n",
    "\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "    \n",
    "num_ftrs = model_ft.classifier[1].in_features\n",
    "model_ft.classifier[1] = nn.Linear(num_ftrs, species_count)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.1, momentum=0.9)\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "#exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 10)\n",
    "\n",
    "if os.path.exists(efficientnetb0_path):\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=8, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft.load_state_dict(torch.load(efficientnetb0_path))\n",
    "    test_model(model_ft, criterion)\n",
    "else:\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=32, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft, val_acc, test_acc, epoch_accs = train_test_save_model(efficientnetb0_path, dataloaders, model_ft, criterion, optimizer_ft, exp_lr_scheduler, 30, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convnet Tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     dataloaders \u001b[38;5;241m=\u001b[39m {x: torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(image_datasets[x], sampler \u001b[38;5;241m=\u001b[39m weighted_samplers[x], batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[1;32m     22\u001b[0m     model_ft\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(conv_path))\n\u001b[0;32m---> 23\u001b[0m     \u001b[43mtest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_ft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     dataloaders \u001b[38;5;241m=\u001b[39m {x: torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(image_datasets[x], sampler \u001b[38;5;241m=\u001b[39m weighted_samplers[x], batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36mtest_model\u001b[0;34m(model, criterion, phase, verbose)\u001b[0m\n\u001b[1;32m     11\u001b[0m     _, preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     12\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 14\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     15\u001b[0m     running_corrects \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(preds \u001b[38;5;241m==\u001b[39m labels\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m     17\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m dataset_sizes[phase]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "conv_path = \"models/92_classifier/conv_tiny_frozen.pt\"\n",
    "\n",
    "model_ft =  models.convnext_tiny(pretrained = True)\n",
    "\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "num_ftrs = model_ft.classifier[2].in_features\n",
    "\n",
    "# Greate Last Layer\n",
    "model_ft.classifier[2] = nn.Linear(num_ftrs, species_count)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.01, momentum=0.9)\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "#exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 3)\n",
    "\n",
    "if os.path.exists(conv_path):\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=8, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft.load_state_dict(torch.load(conv_path))\n",
    "    test_model(model_ft, criterion)\n",
    "else:\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=32, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft, val_acc, test_acc, epoch_accs = train_test_save_model(conv_path, dataloaders, model_ft, criterion, optimizer_ft, exp_lr_scheduler, 30, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Several Layers UnFrozen\n",
    "### Resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18_path = \"models/92_classifier/rn18_frozen_partial.pt\"\n",
    "\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model_ft.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, species_count)\n",
    "model_ft = model_ft.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.1, momentum=0.9)\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "#exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "PytorchStreamReader failed reading zip archive: failed finding central directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(resnet18_path):\n\u001b[1;32m      2\u001b[0m     dataloaders \u001b[38;5;241m=\u001b[39m {x: torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(image_datasets[x], sampler \u001b[38;5;241m=\u001b[39m weighted_samplers[x], batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[0;32m----> 3\u001b[0m     model_ft\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresnet18_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      4\u001b[0m     test_model(model_ft, criterion)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/serialization.py:705\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    701\u001b[0m     \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n\u001b[0;32m--> 705\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_reader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _is_torchscript_zip(opened_zipfile):\n\u001b[1;32m    707\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m received a zip file that looks like a TorchScript archive\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    708\u001b[0m                           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m dispatching to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.jit.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (call \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.jit.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directly to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    709\u001b[0m                           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m silence this warning)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mUserWarning\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/serialization.py:243\u001b[0m, in \u001b[0;36m_open_zipfile_reader.__init__\u001b[0;34m(self, name_or_buffer)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name_or_buffer) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 243\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_zipfile_reader, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: PytorchStreamReader failed reading zip archive: failed finding central directory"
     ]
    }
   ],
   "source": [
    "if os.path.exists(resnet18_path):\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=64, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft.load_state_dict(torch.load(resnet18_path))\n",
    "    test_model(model_ft, criterion)\n",
    "else:\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=128, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft, val_acc, test_acc, epoch_accs = train_test_save_model(resnet18_path, dataloaders, model_ft, criterion, optimizer_ft, exp_lr_scheduler, 30, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnetb0_path = \"models/92_classifier/en_netb0_frozen_partial.pt\"\n",
    "\n",
    "model_ft =  models.efficientnet_b0(pretrained = True)\n",
    "\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model_ft.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_ft.avgpool.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_ft.features[7].parameters():\n",
    "    param.requires_grad = True\n",
    "# Create Last Layer\n",
    "    \n",
    "num_ftrs = model_ft.classifier[1].in_features\n",
    "model_ft.classifier[1] = nn.Linear(num_ftrs, species_count)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.1, momentum=0.9)\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "#exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 1.9392 Acc: 0.6396\n"
     ]
    }
   ],
   "source": [
    "build_from_path(model_ft, efficientnetb0_path, train_batch=8, test_batch=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convnet Tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft =  models.convnext_tiny(pretrained = True)\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model_ft.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_ft.avgpool.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_ft.features[7].parameters():\n",
    "    param.requires_grad = True\n",
    "# Create Last Layer\n",
    "num_ftrs = model_ft.classifier[2].in_features\n",
    "model_ft.classifier[2] = nn.Linear(num_ftrs, species_count)\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.01, momentum=0.9)\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 1.2114 Acc: 0.7242\n"
     ]
    }
   ],
   "source": [
    "conv_path = \"models/92_classifier/conv_tiny_frozen_partial.pt\"\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "build_from_path(model_ft, conv_path, train_batch=8, test_batch=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 1.1975 Acc: 0.7387\n"
     ]
    }
   ],
   "source": [
    "conv_path = \"models/92_classifier/conv_tiny_frozen_partial_decay.pt\"\n",
    "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 5)\n",
    "build_from_path(model_ft, conv_path, train_batch=8, test_batch=32, decay = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freezing Variations with Convnet Tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load already trained model\n",
    "model_ft =  models.convnext_tiny(pretrained = True)\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model_ft.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_ft.avgpool.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_ft.features[7].parameters():\n",
    "    param.requires_grad = True\n",
    "# Create Last Layer\n",
    "num_ftrs = model_ft.classifier[2].in_features\n",
    "model_ft.classifier[2] = nn.Linear(num_ftrs, species_count)\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "model_ft.load_state_dict(torch.load(\"models/92_classifier/conv_tiny_frozen_partial_decay.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6th Block+ Unfrozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unfreeze the next convolutional block\n",
    "for param in model_ft.features[6].parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 1.1063 Acc: 0.7632\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "conv_path = \"models/92_classifier/conv_tiny_frozen_partial2_decay.pt\"\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 5)\n",
    "build_from_path(model_ft, conv_path, train_batch=8, test_batch=32, decay = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5th Block+ Unfrozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load already trained model\n",
    "model_ft =  models.convnext_tiny(pretrained = True)\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model_ft.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_ft.avgpool.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_ft.features[7].parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_ft.features[6].parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_ft.features[5].parameters():\n",
    "    param.requires_grad = True\n",
    "# Create Last Layer\n",
    "num_ftrs = model_ft.classifier[2].in_features\n",
    "model_ft.classifier[2] = nn.Linear(num_ftrs, species_count)\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "model_ft.load_state_dict(torch.load(\"models/92_classifier/conv_tiny_frozen_partial2_decay.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 1.1083 Acc: 0.7493\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "conv_path = \"models/92_classifier/conv_tiny_frozen_partial3_decay.pt\"\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 5)\n",
    "build_from_path(model_ft, conv_path, train_batch=8, test_batch=32, decay = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6th Block+ Unfrozen from the Outset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load already trained model\n",
    "model_ft =  models.convnext_tiny(pretrained = True)\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model_ft.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_ft.avgpool.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_ft.features[7].parameters():\n",
    "    param.requires_grad = True\n",
    "#Unfreeze the next convolutional block\n",
    "for param in model_ft.features[6].parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "# Create Last Layer\n",
    "num_ftrs = model_ft.classifier[2].in_features\n",
    "model_ft.classifier[2] = nn.Linear(num_ftrs, species_count)\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.9501 Acc: 0.7894\n"
     ]
    }
   ],
   "source": [
    "conv_path = \"models/92_classifier/conv_tiny_frozen_partial2a_decay.pt\"\n",
    "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 5)\n",
    "build_from_path(model_ft, conv_path, train_batch=8, test_batch=32, decay = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combatting Overfitting\n",
    "### Ridge Regression\n",
    "https://stackoverflow.com/questions/42704283/adding-l1-l2-regularization-in-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WD = 0.0002\n",
      "0.0002\n",
      "Epoch 1/30\n",
      "----------\n",
      "train Loss: 2.1078 Acc: 0.4803\n",
      "val Loss: 1.2150 Acc: 0.6619\n",
      "\n",
      "Epoch 2/30\n",
      "----------\n",
      "train Loss: 1.1754 Acc: 0.6995\n",
      "val Loss: 1.0206 Acc: 0.7039\n",
      "\n",
      "Epoch 3/30\n",
      "----------\n",
      "train Loss: 0.8713 Acc: 0.7732\n",
      "val Loss: 0.9808 Acc: 0.7425\n",
      "\n",
      "Epoch 4/30\n",
      "----------\n",
      "train Loss: 0.6974 Acc: 0.8164\n",
      "val Loss: 1.0513 Acc: 0.7209\n",
      "\n",
      "Epoch 5/30\n",
      "----------\n",
      "train Loss: 0.5519 Acc: 0.8538\n",
      "val Loss: 0.9750 Acc: 0.7493\n",
      "\n",
      "Epoch 6/30\n",
      "----------\n",
      "train Loss: 0.4534 Acc: 0.8785\n",
      "val Loss: 0.9472 Acc: 0.7589\n",
      "\n",
      "Epoch 7/30\n",
      "----------\n",
      "train Loss: 0.3714 Acc: 0.8992\n",
      "val Loss: 0.9437 Acc: 0.7657\n",
      "\n",
      "Epoch 8/30\n",
      "----------\n",
      "train Loss: 0.3205 Acc: 0.9144\n",
      "val Loss: 0.9066 Acc: 0.7771\n",
      "\n",
      "Epoch 9/30\n",
      "----------\n",
      "train Loss: 0.2759 Acc: 0.9253\n",
      "val Loss: 0.9239 Acc: 0.7896\n",
      "\n",
      "Epoch 10/30\n",
      "----------\n",
      "train Loss: 0.2558 Acc: 0.9320\n",
      "val Loss: 0.8371 Acc: 0.7839\n",
      "\n",
      "Epoch 11/30\n",
      "----------\n",
      "train Loss: 0.2270 Acc: 0.9397\n",
      "val Loss: 1.0880 Acc: 0.7357\n",
      "\n",
      "Epoch 12/30\n",
      "----------\n",
      "train Loss: 0.2017 Acc: 0.9468\n",
      "val Loss: 1.0541 Acc: 0.7516\n",
      "\n",
      "Epoch 13/30\n",
      "----------\n",
      "train Loss: 0.1858 Acc: 0.9514\n",
      "val Loss: 0.8652 Acc: 0.7867\n",
      "\n",
      "Epoch 14/30\n",
      "----------\n",
      "train Loss: 0.1689 Acc: 0.9567\n",
      "val Loss: 0.8313 Acc: 0.8020\n",
      "\n",
      "Epoch 15/30\n",
      "----------\n",
      "train Loss: 0.1648 Acc: 0.9561\n",
      "val Loss: 0.9747 Acc: 0.7748\n",
      "\n",
      "Epoch 16/30\n",
      "----------\n",
      "train Loss: 0.1666 Acc: 0.9577\n",
      "val Loss: 0.9331 Acc: 0.7833\n",
      "\n",
      "Epoch 17/30\n",
      "----------\n",
      "train Loss: 0.1530 Acc: 0.9603\n",
      "val Loss: 0.9900 Acc: 0.7737\n",
      "\n",
      "Epoch 18/30\n",
      "----------\n",
      "train Loss: 0.1518 Acc: 0.9606\n",
      "val Loss: 0.9144 Acc: 0.7901\n",
      "\n",
      "Epoch 19/30\n",
      "----------\n",
      "train Loss: 0.1433 Acc: 0.9624\n",
      "val Loss: 1.0135 Acc: 0.7754\n",
      "\n",
      "Epoch 20/30\n",
      "----------\n",
      "train Loss: 0.1371 Acc: 0.9636\n",
      "val Loss: 1.0334 Acc: 0.7584\n",
      "\n",
      "Epoch 21/30\n",
      "----------\n",
      "train Loss: 0.1273 Acc: 0.9662\n",
      "val Loss: 0.8760 Acc: 0.8106\n",
      "\n",
      "Epoch 22/30\n",
      "----------\n",
      "train Loss: 0.1294 Acc: 0.9651\n",
      "val Loss: 0.8820 Acc: 0.7935\n",
      "\n",
      "Epoch 23/30\n",
      "----------\n",
      "train Loss: 0.1285 Acc: 0.9655\n",
      "val Loss: 1.1192 Acc: 0.7448\n",
      "\n",
      "Epoch 24/30\n",
      "----------\n",
      "train Loss: 0.1196 Acc: 0.9671\n",
      "val Loss: 0.9400 Acc: 0.7691\n",
      "\n",
      "Epoch 25/30\n",
      "----------\n",
      "train Loss: 0.1229 Acc: 0.9668\n",
      "val Loss: 1.0258 Acc: 0.7811\n",
      "\n",
      "Epoch 26/30\n",
      "----------\n",
      "train Loss: 0.1239 Acc: 0.9663\n",
      "val Loss: 1.0908 Acc: 0.7595\n",
      "\n",
      "Epoch 27/30\n",
      "----------\n",
      "train Loss: 0.1151 Acc: 0.9690\n",
      "val Loss: 0.9209 Acc: 0.7930\n",
      "\n",
      "Epoch 28/30\n",
      "----------\n",
      "train Loss: 0.1225 Acc: 0.9661\n",
      "val Loss: 0.9727 Acc: 0.7913\n",
      "\n",
      "Epoch 29/30\n",
      "----------\n",
      "train Loss: 0.1135 Acc: 0.9691\n",
      "val Loss: 0.9559 Acc: 0.7742\n",
      "\n",
      "Epoch 30/30\n",
      "----------\n",
      "train Loss: 0.1199 Acc: 0.9672\n",
      "val Loss: 0.9813 Acc: 0.7782\n",
      "\n",
      "Training complete in 45m 20s\n",
      "Best val Acc: 0.810550\n",
      "Val Accuracy: 0.8105501985252411\n",
      "Test Accuracy: -1\n",
      "test Loss: 0.9486 Acc: 0.7861\n",
      "WD = 7e-05\n",
      "7e-05\n",
      "Epoch 1/30\n",
      "----------\n",
      "train Loss: 2.1037 Acc: 0.4810\n",
      "val Loss: 1.3596 Acc: 0.6313\n",
      "\n",
      "Epoch 2/30\n",
      "----------\n",
      "train Loss: 1.1705 Acc: 0.6981\n",
      "val Loss: 1.0909 Acc: 0.7130\n",
      "\n",
      "Epoch 3/30\n",
      "----------\n",
      "train Loss: 0.8872 Acc: 0.7682\n",
      "val Loss: 1.0613 Acc: 0.7317\n",
      "\n",
      "Epoch 4/30\n",
      "----------\n",
      "train Loss: 0.6866 Acc: 0.8179\n",
      "val Loss: 0.9806 Acc: 0.7300\n",
      "\n",
      "Epoch 5/30\n",
      "----------\n",
      "train Loss: 0.5493 Acc: 0.8547\n",
      "val Loss: 0.9297 Acc: 0.7714\n",
      "\n",
      "Epoch 6/30\n",
      "----------\n",
      "train Loss: 0.4412 Acc: 0.8802\n",
      "val Loss: 1.0846 Acc: 0.7396\n",
      "\n",
      "Epoch 7/30\n",
      "----------\n",
      "train Loss: 0.3674 Acc: 0.9019\n",
      "val Loss: 1.1095 Acc: 0.7578\n",
      "\n",
      "Epoch 8/30\n",
      "----------\n",
      "train Loss: 0.3093 Acc: 0.9169\n",
      "val Loss: 1.0048 Acc: 0.7708\n",
      "\n",
      "Epoch 9/30\n",
      "----------\n",
      "train Loss: 0.2605 Acc: 0.9313\n",
      "val Loss: 1.0164 Acc: 0.7850\n",
      "\n",
      "Epoch 10/30\n",
      "----------\n",
      "train Loss: 0.2356 Acc: 0.9379\n",
      "val Loss: 1.0289 Acc: 0.7833\n",
      "\n",
      "Epoch 11/30\n",
      "----------\n",
      "train Loss: 0.2175 Acc: 0.9425\n",
      "val Loss: 1.1398 Acc: 0.7623\n",
      "\n",
      "Epoch 12/30\n",
      "----------\n",
      "train Loss: 0.1924 Acc: 0.9488\n",
      "val Loss: 1.0813 Acc: 0.7640\n",
      "\n",
      "Epoch 13/30\n",
      "----------\n",
      "train Loss: 0.1755 Acc: 0.9520\n",
      "val Loss: 0.9815 Acc: 0.7867\n",
      "\n",
      "Epoch 14/30\n",
      "----------\n",
      "train Loss: 0.1678 Acc: 0.9547\n",
      "val Loss: 1.0806 Acc: 0.7652\n",
      "\n",
      "Epoch 15/30\n",
      "----------\n",
      "train Loss: 0.1557 Acc: 0.9589\n",
      "val Loss: 1.0609 Acc: 0.7635\n",
      "\n",
      "Epoch 16/30\n",
      "----------\n",
      "train Loss: 0.1478 Acc: 0.9599\n",
      "val Loss: 0.9792 Acc: 0.7833\n",
      "\n",
      "Epoch 17/30\n",
      "----------\n",
      "train Loss: 0.1411 Acc: 0.9631\n",
      "val Loss: 1.0165 Acc: 0.7811\n",
      "\n",
      "Epoch 18/30\n",
      "----------\n",
      "train Loss: 0.1211 Acc: 0.9670\n",
      "val Loss: 1.1723 Acc: 0.7674\n",
      "\n",
      "Epoch 19/30\n",
      "----------\n",
      "train Loss: 0.1327 Acc: 0.9639\n",
      "val Loss: 1.0770 Acc: 0.7771\n",
      "\n",
      "Epoch 20/30\n",
      "----------\n",
      "train Loss: 0.1172 Acc: 0.9678\n",
      "val Loss: 0.9674 Acc: 0.7862\n",
      "\n",
      "Epoch 21/30\n",
      "----------\n",
      "train Loss: 0.1050 Acc: 0.9704\n",
      "val Loss: 0.9944 Acc: 0.7941\n",
      "\n",
      "Epoch 22/30\n",
      "----------\n",
      "train Loss: 0.1042 Acc: 0.9708\n",
      "val Loss: 0.9347 Acc: 0.8020\n",
      "\n",
      "Epoch 23/30\n",
      "----------\n",
      "train Loss: 0.1034 Acc: 0.9709\n",
      "val Loss: 0.9813 Acc: 0.7839\n",
      "\n",
      "Epoch 24/30\n",
      "----------\n",
      "train Loss: 0.0999 Acc: 0.9709\n",
      "val Loss: 1.0681 Acc: 0.7839\n",
      "\n",
      "Epoch 25/30\n",
      "----------\n",
      "train Loss: 0.0980 Acc: 0.9715\n",
      "val Loss: 0.9783 Acc: 0.7998\n",
      "\n",
      "Epoch 26/30\n",
      "----------\n",
      "train Loss: 0.0947 Acc: 0.9717\n",
      "val Loss: 1.0666 Acc: 0.7725\n",
      "\n",
      "Epoch 27/30\n",
      "----------\n",
      "train Loss: 0.0926 Acc: 0.9726\n",
      "val Loss: 1.1269 Acc: 0.7742\n",
      "\n",
      "Epoch 28/30\n",
      "----------\n",
      "train Loss: 0.0895 Acc: 0.9736\n",
      "val Loss: 1.1780 Acc: 0.7742\n",
      "\n",
      "Epoch 29/30\n",
      "----------\n",
      "train Loss: 0.0776 Acc: 0.9755\n",
      "val Loss: 1.1544 Acc: 0.7686\n",
      "\n",
      "Epoch 30/30\n",
      "----------\n",
      "train Loss: 0.0843 Acc: 0.9740\n",
      "val Loss: 1.1011 Acc: 0.7663\n",
      "\n",
      "Training complete in 44m 25s\n",
      "Best val Acc: 0.802042\n",
      "Val Accuracy: 0.8020419739081113\n",
      "Test Accuracy: -1\n",
      "test Loss: 1.0916 Acc: 0.7671\n"
     ]
    }
   ],
   "source": [
    "conv_path = \"models/92_classifier/ctf_p2ad_dropout.pt\"\n",
    "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 5)\n",
    "for wd in [0.0002, 0.00007]:#[0.0005, 0.0003, 0.0001, 0.00005, 0.00001]:\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"WD = {wd}\")\n",
    "    print(wd)\n",
    "    try:\n",
    "        conv_path_s = \"models/92_classifier/ctf_p2ad_dropout_\" + str(str(wd).split(\".\")[1] )+ \".pt\"\n",
    "    except:\n",
    "         conv_path_s = \"models/92_classifier/ctf_p2ad_dropout_\" + str(wd)+ \".pt\"\n",
    "    \n",
    "    # Load already trained model\n",
    "    model_ft =  models.convnext_tiny(pretrained = True)\n",
    "    for param in model_ft.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model_ft.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in model_ft.avgpool.parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in model_ft.features[7].parameters():\n",
    "        param.requires_grad = True\n",
    "    #Unfreeze the next convolutional block\n",
    "    for param in model_ft.features[6].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "\n",
    "\n",
    "    # Create Last Layer\n",
    "    num_ftrs = model_ft.classifier[2].in_features\n",
    "    model_ft.classifier[2] = nn.Linear(num_ftrs, species_count)\n",
    "    model_ft = model_ft.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.01, momentum=0.9, weight_decay=wd)\n",
    "    \n",
    "    build_from_path(model_ft, conv_path_s, train_batch=8, test_batch=32, decay = True)\n",
    "    build_from_path(model_ft, conv_path_s, train_batch=8, test_batch=32, decay = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Training Sets\n",
    "### Add Random Crops\n",
    "- https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0 \n",
    "- https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0/tables/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We normalize to imagenet mean for the data (https://stackoverflow.com/questions/58151507/why-pytorch-officially-use-mean-0-485-0-456-0-406-and-std-0-229-0-224-0-2)\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomResizedCrop(size=(224, 224)),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        transforms.Resize([224, 224])]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        transforms.Resize([224, 224])])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dirs = [\"/home/shivaram/DS/Projects/FishID/data/model_data/is_fish_scientific/\", \"/home/shivaram/DS/Projects/FishID/data/model_data/is_fish_common/\", \"/home/shivaram/DS/Projects/FishID/data/model_data/is_fish_mixed/\"]\n",
    "diff_image_datasets = {dd:{x: datasets.ImageFolder(os.path.join(dd, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val', 'test']} for dd in data_dirs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = diff_image_datasets[\"/home/shivaram/DS/Projects/FishID/data/model_data/is_fish_scientific/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_samplers = {}\n",
    "for subset in [\"train\", \"val\", \"test\"]:\n",
    "    target = image_datasets[subset].targets\n",
    "    \n",
    "    if True:#subset == \"train\":\n",
    "        class_sample_count =np.array([ len(np.where(target == t)[0]) for t in np.unique(target)])\n",
    "        weight = 1. / class_sample_count\n",
    "        samples_weight = np.array([weight[t] for t in target])\n",
    "        samples_weight = torch.from_numpy(samples_weight)\n",
    "        epoch_samples = 2560#len(samples_weight)\n",
    "        sampler = data.WeightedRandomSampler(samples_weight, epoch_samples)\n",
    "        weighted_samplers[subset] = sampler\n",
    "    else:\n",
    "        sampler = data.RandomSampler(image_datasets[subset])\n",
    "        weighted_samplers[subset] = sampler\n",
    "    \n",
    "\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], \n",
    "                                              batch_size=batch_size, num_workers=4)\n",
    "              for x in ['train', 'val', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "class_names = image_datasets['train'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load already trained model\n",
    "model_ft =  models.convnext_tiny(pretrained = True)\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model_ft.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_ft.avgpool.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_ft.features[7].parameters():\n",
    "    param.requires_grad = True\n",
    "#Unfreeze the next convolutional block\n",
    "for param in model_ft.features[6].parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "# Create Last Layer\n",
    "num_ftrs = model_ft.classifier[2].in_features\n",
    "model_ft.classifier[2] = nn.Linear(num_ftrs, species_count)\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.9154 Acc: 0.8045\n"
     ]
    }
   ],
   "source": [
    "conv_path = \"models/92_classifier/ct_fp2ad_random_crop.pt\"\n",
    "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 5)\n",
    "build_from_path(model_ft, conv_path, train_batch=8, test_batch=32, decay = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = diff_image_datasets[\"/home/shivaram/DS/Projects/FishID/data/model_data/is_fish_common/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_samplers = {}\n",
    "for subset in [\"train\", \"val\", \"test\"]:\n",
    "    target = image_datasets[subset].targets\n",
    "    \n",
    "    if True:#subset == \"train\":\n",
    "        class_sample_count =np.array([ len(np.where(target == t)[0]) for t in np.unique(target)])\n",
    "        weight = 1. / class_sample_count\n",
    "        samples_weight = np.array([weight[t] for t in target])\n",
    "        samples_weight = torch.from_numpy(samples_weight)\n",
    "        epoch_samples = len(samples_weight)\n",
    "        sampler = data.WeightedRandomSampler(samples_weight, epoch_samples)\n",
    "        weighted_samplers[subset] = sampler\n",
    "    else:\n",
    "        sampler = data.RandomSampler(image_datasets[subset])\n",
    "        weighted_samplers[subset] = sampler\n",
    "    \n",
    "\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], \n",
    "                                              batch_size=batch_size, num_workers=4)\n",
    "              for x in ['train', 'val', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "class_names = image_datasets['train'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load already trained model\n",
    "model_ft =  models.convnext_tiny(pretrained = True)\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model_ft.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_ft.avgpool.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_ft.features[7].parameters():\n",
    "    param.requires_grad = True\n",
    "#Unfreeze the next convolutional block\n",
    "for param in model_ft.features[6].parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "# Create Last Layer\n",
    "num_ftrs = model_ft.classifier[2].in_features\n",
    "model_ft.classifier[2] = nn.Linear(num_ftrs, species_count)\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "----------\n",
      "train Loss: 2.0278 Acc: 0.4943\n",
      "val Loss: 1.2706 Acc: 0.6500\n",
      "\n",
      "Epoch 2/30\n",
      "----------\n",
      "train Loss: 1.1480 Acc: 0.7038\n",
      "val Loss: 1.0384 Acc: 0.7198\n",
      "\n",
      "Epoch 3/30\n",
      "----------\n",
      "train Loss: 0.8629 Acc: 0.7733\n",
      "val Loss: 0.9458 Acc: 0.7374\n",
      "\n",
      "Epoch 4/30\n",
      "----------\n",
      "train Loss: 0.6925 Acc: 0.8175\n",
      "val Loss: 1.0147 Acc: 0.7408\n",
      "\n",
      "Epoch 5/30\n",
      "----------\n",
      "train Loss: 0.5393 Acc: 0.8558\n",
      "val Loss: 0.9460 Acc: 0.7572\n",
      "\n",
      "Epoch 6/30\n",
      "----------\n",
      "train Loss: 0.4410 Acc: 0.8807\n",
      "val Loss: 0.9844 Acc: 0.7561\n",
      "\n",
      "Epoch 7/30\n",
      "----------\n",
      "train Loss: 0.3489 Acc: 0.9050\n",
      "val Loss: 1.0599 Acc: 0.7805\n",
      "\n",
      "Epoch 8/30\n",
      "----------\n",
      "train Loss: 0.2982 Acc: 0.9199\n",
      "val Loss: 1.1262 Acc: 0.7499\n",
      "\n",
      "Epoch 9/30\n",
      "----------\n",
      "train Loss: 0.2612 Acc: 0.9294\n",
      "val Loss: 0.9631 Acc: 0.7794\n",
      "\n",
      "Epoch 10/30\n",
      "----------\n",
      "train Loss: 0.1683 Acc: 0.9537\n",
      "val Loss: 0.9246 Acc: 0.7867\n",
      "\n",
      "Epoch 11/30\n",
      "----------\n",
      "train Loss: 0.1389 Acc: 0.9605\n",
      "val Loss: 0.9687 Acc: 0.7771\n",
      "\n",
      "Epoch 12/30\n",
      "----------\n",
      "train Loss: 0.1198 Acc: 0.9653\n",
      "val Loss: 0.9808 Acc: 0.7737\n",
      "\n",
      "Epoch 13/30\n",
      "----------\n",
      "train Loss: 0.1065 Acc: 0.9691\n",
      "val Loss: 0.9584 Acc: 0.7884\n",
      "\n",
      "Epoch 14/30\n",
      "----------\n",
      "train Loss: 0.1052 Acc: 0.9688\n",
      "val Loss: 0.9101 Acc: 0.8020\n",
      "\n",
      "Epoch 15/30\n",
      "----------\n",
      "train Loss: 0.1000 Acc: 0.9700\n",
      "val Loss: 0.9580 Acc: 0.7952\n",
      "\n",
      "Epoch 16/30\n",
      "----------\n",
      "train Loss: 0.0931 Acc: 0.9723\n",
      "val Loss: 0.9547 Acc: 0.7845\n",
      "\n",
      "Epoch 17/30\n",
      "----------\n",
      "train Loss: 0.0924 Acc: 0.9724\n",
      "val Loss: 0.9152 Acc: 0.7941\n",
      "\n",
      "Epoch 18/30\n",
      "----------\n",
      "train Loss: 0.0884 Acc: 0.9733\n",
      "val Loss: 1.0435 Acc: 0.7839\n",
      "\n",
      "Epoch 19/30\n",
      "----------\n",
      "train Loss: 0.0814 Acc: 0.9749\n",
      "val Loss: 0.9386 Acc: 0.7907\n",
      "\n",
      "Epoch 20/30\n",
      "----------\n",
      "train Loss: 0.0818 Acc: 0.9748\n",
      "val Loss: 0.9119 Acc: 0.7958\n",
      "\n",
      "Epoch 21/30\n",
      "----------\n",
      "train Loss: 0.0737 Acc: 0.9773\n",
      "val Loss: 1.0440 Acc: 0.7822\n",
      "\n",
      "Epoch 22/30\n",
      "----------\n",
      "train Loss: 0.0733 Acc: 0.9763\n",
      "val Loss: 0.8795 Acc: 0.7992\n",
      "\n",
      "Epoch 23/30\n",
      "----------\n",
      "train Loss: 0.0708 Acc: 0.9770\n",
      "val Loss: 0.9818 Acc: 0.8043\n",
      "\n",
      "Epoch 24/30\n",
      "----------\n",
      "train Loss: 0.0723 Acc: 0.9766\n",
      "val Loss: 0.9507 Acc: 0.7981\n",
      "\n",
      "Epoch 25/30\n",
      "----------\n",
      "train Loss: 0.0691 Acc: 0.9780\n",
      "val Loss: 0.9642 Acc: 0.7890\n",
      "\n",
      "Epoch 26/30\n",
      "----------\n",
      "train Loss: 0.0668 Acc: 0.9782\n",
      "val Loss: 0.9623 Acc: 0.7907\n",
      "\n",
      "Epoch 27/30\n",
      "----------\n",
      "train Loss: 0.0647 Acc: 0.9778\n",
      "val Loss: 0.9491 Acc: 0.7901\n",
      "\n",
      "Epoch 28/30\n",
      "----------\n",
      "train Loss: 0.0649 Acc: 0.9787\n",
      "val Loss: 0.9317 Acc: 0.7907\n",
      "\n",
      "Epoch 29/30\n",
      "----------\n",
      "train Loss: 0.0670 Acc: 0.9776\n",
      "val Loss: 0.9224 Acc: 0.8066\n",
      "\n",
      "Epoch 30/30\n",
      "----------\n",
      "train Loss: 0.0610 Acc: 0.9793\n",
      "val Loss: 0.9440 Acc: 0.7964\n",
      "\n",
      "Training complete in 44m 3s\n",
      "Best val Acc: 0.806580\n",
      "Val Accuracy: 0.8065796937039138\n",
      "Test Accuracy: -1\n"
     ]
    }
   ],
   "source": [
    "conv_path = \"models/92_classifier/ct_fp2ad_common.pt\"\n",
    "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 5)\n",
    "build_from_path(model_ft, conv_path, train_batch=8, test_batch=32, decay = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.9042 Acc: 0.8050\n"
     ]
    }
   ],
   "source": [
    "build_from_path(model_ft, conv_path, train_batch=8, test_batch=32, decay = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = diff_image_datasets[\"/home/shivaram/DS/Projects/FishID/data/model_data/is_fish_mixed/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_samplers = {}\n",
    "for subset in [\"train\", \"val\", \"test\"]:\n",
    "    target = image_datasets[subset].targets\n",
    "    \n",
    "    if True:#subset == \"train\":\n",
    "        class_sample_count =np.array([ len(np.where(target == t)[0]) for t in np.unique(target)])\n",
    "        weight = 1. / class_sample_count\n",
    "        samples_weight = np.array([weight[t] for t in target])\n",
    "        samples_weight = torch.from_numpy(samples_weight)\n",
    "        epoch_samples = len(samples_weight)\n",
    "        sampler = data.WeightedRandomSampler(samples_weight, epoch_samples)\n",
    "        weighted_samplers[subset] = sampler\n",
    "    else:\n",
    "        sampler = data.RandomSampler(image_datasets[subset])\n",
    "        weighted_samplers[subset] = sampler\n",
    "    \n",
    "\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], \n",
    "                                              batch_size=batch_size, num_workers=4)\n",
    "              for x in ['train', 'val', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "class_names = image_datasets['train'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load already trained model\n",
    "model_ft =  models.convnext_tiny(pretrained = True)\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model_ft.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_ft.avgpool.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_ft.features[7].parameters():\n",
    "    param.requires_grad = True\n",
    "#Unfreeze the next convolutional block\n",
    "for param in model_ft.features[6].parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "# Create Last Layer\n",
    "num_ftrs = model_ft.classifier[2].in_features\n",
    "model_ft.classifier[2] = nn.Linear(num_ftrs, species_count)\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "----------\n",
      "train Loss: 1.0990 Acc: 0.2720\n",
      "val Loss: 1.1849 Acc: 0.6988\n",
      "\n",
      "Epoch 2/200\n",
      "----------\n",
      "train Loss: 0.6199 Acc: 0.3863\n",
      "val Loss: 1.0847 Acc: 0.7050\n",
      "\n",
      "Epoch 3/200\n",
      "----------\n",
      "train Loss: 0.4900 Acc: 0.4177\n",
      "val Loss: 0.9415 Acc: 0.7425\n",
      "\n",
      "Epoch 4/200\n",
      "----------\n",
      "train Loss: 0.3692 Acc: 0.4473\n",
      "val Loss: 0.9718 Acc: 0.7629\n",
      "\n",
      "Epoch 5/200\n",
      "----------\n",
      "train Loss: 0.2837 Acc: 0.4692\n",
      "val Loss: 1.0345 Acc: 0.7436\n",
      "\n",
      "Epoch 6/200\n",
      "----------\n",
      "train Loss: 0.2442 Acc: 0.4799\n",
      "val Loss: 1.0146 Acc: 0.7533\n",
      "\n",
      "Epoch 7/200\n",
      "----------\n",
      "train Loss: 0.1993 Acc: 0.4926\n",
      "val Loss: 0.9447 Acc: 0.7805\n",
      "\n",
      "Epoch 8/200\n",
      "----------\n",
      "train Loss: 0.1711 Acc: 0.4997\n",
      "val Loss: 0.9685 Acc: 0.7652\n",
      "\n",
      "Epoch 9/200\n",
      "----------\n",
      "train Loss: 0.1437 Acc: 0.5072\n",
      "val Loss: 0.9601 Acc: 0.7788\n",
      "\n",
      "Epoch 10/200\n",
      "----------\n",
      "train Loss: 0.1230 Acc: 0.5128\n",
      "val Loss: 1.0572 Acc: 0.7731\n",
      "\n",
      "Epoch 11/200\n",
      "----------\n",
      "train Loss: 0.1154 Acc: 0.5150\n",
      "val Loss: 0.9641 Acc: 0.7760\n",
      "\n",
      "Epoch 12/200\n",
      "----------\n",
      "train Loss: 0.1012 Acc: 0.5181\n",
      "val Loss: 1.0127 Acc: 0.7731\n",
      "\n",
      "Epoch 13/200\n",
      "----------\n",
      "train Loss: 0.0893 Acc: 0.5219\n",
      "val Loss: 1.0394 Acc: 0.7765\n",
      "\n",
      "Epoch 14/200\n",
      "----------\n",
      "train Loss: 0.0883 Acc: 0.5220\n",
      "val Loss: 1.0433 Acc: 0.7555\n",
      "\n",
      "Epoch 15/200\n",
      "----------\n",
      "train Loss: 0.0609 Acc: 0.5288\n",
      "val Loss: 0.9816 Acc: 0.7805\n",
      "\n",
      "Epoch 16/200\n",
      "----------\n",
      "train Loss: 0.0522 Acc: 0.5311\n",
      "val Loss: 0.9828 Acc: 0.7964\n",
      "\n",
      "Epoch 17/200\n",
      "----------\n",
      "train Loss: 0.0475 Acc: 0.5320\n",
      "val Loss: 0.8619 Acc: 0.8196\n",
      "\n",
      "Epoch 18/200\n",
      "----------\n",
      "train Loss: 0.0417 Acc: 0.5332\n",
      "val Loss: 0.9276 Acc: 0.8020\n",
      "\n",
      "Epoch 19/200\n",
      "----------\n",
      "train Loss: 0.0440 Acc: 0.5322\n",
      "val Loss: 1.0954 Acc: 0.7737\n",
      "\n",
      "Epoch 20/200\n",
      "----------\n",
      "train Loss: 0.0408 Acc: 0.5329\n",
      "val Loss: 0.8361 Acc: 0.8242\n",
      "\n",
      "Epoch 21/200\n",
      "----------\n",
      "train Loss: 0.0388 Acc: 0.5338\n",
      "val Loss: 0.8918 Acc: 0.8106\n",
      "\n",
      "Epoch 22/200\n",
      "----------\n",
      "train Loss: 0.0371 Acc: 0.5334\n",
      "val Loss: 0.9603 Acc: 0.8015\n",
      "\n",
      "Epoch 23/200\n",
      "----------\n",
      "train Loss: 0.0374 Acc: 0.5335\n",
      "val Loss: 0.9362 Acc: 0.8032\n",
      "\n",
      "Epoch 24/200\n",
      "----------\n",
      "train Loss: 0.0357 Acc: 0.5346\n",
      "val Loss: 0.9628 Acc: 0.8020\n",
      "\n",
      "Epoch 25/200\n",
      "----------\n",
      "train Loss: 0.0355 Acc: 0.5336\n",
      "val Loss: 1.0326 Acc: 0.7873\n",
      "\n",
      "Epoch 26/200\n",
      "----------\n",
      "train Loss: 0.0350 Acc: 0.5344\n",
      "val Loss: 0.9213 Acc: 0.8026\n",
      "\n",
      "Epoch 27/200\n",
      "----------\n",
      "train Loss: 0.0339 Acc: 0.5347\n",
      "val Loss: 0.9462 Acc: 0.8100\n",
      "\n",
      "Epoch 28/200\n",
      "----------\n",
      "train Loss: 0.0332 Acc: 0.5346\n",
      "val Loss: 1.0054 Acc: 0.7930\n",
      "\n",
      "Epoch 29/200\n",
      "----------\n",
      "train Loss: 0.0304 Acc: 0.5360\n",
      "val Loss: 1.0388 Acc: 0.7975\n",
      "\n",
      "Epoch 30/200\n",
      "----------\n",
      "train Loss: 0.0321 Acc: 0.5351\n",
      "val Loss: 0.8373 Acc: 0.8162\n",
      "\n",
      "Epoch 31/200\n",
      "----------\n",
      "train Loss: 0.0318 Acc: 0.5355\n",
      "val Loss: 0.9728 Acc: 0.8060\n",
      "\n",
      "Epoch 32/200\n",
      "----------\n",
      "train Loss: 0.0301 Acc: 0.5357\n",
      "val Loss: 0.7960 Acc: 0.8276\n",
      "\n",
      "Epoch 33/200\n",
      "----------\n",
      "train Loss: 0.0266 Acc: 0.5362\n",
      "val Loss: 1.0203 Acc: 0.7952\n",
      "\n",
      "Epoch 34/200\n",
      "----------\n",
      "train Loss: 0.0271 Acc: 0.5359\n",
      "val Loss: 0.9120 Acc: 0.8106\n",
      "\n",
      "Epoch 35/200\n",
      "----------\n",
      "train Loss: 0.0253 Acc: 0.5368\n",
      "val Loss: 0.9224 Acc: 0.8196\n",
      "\n",
      "Epoch 36/200\n",
      "----------\n",
      "train Loss: 0.0285 Acc: 0.5356\n",
      "val Loss: 0.9982 Acc: 0.7969\n",
      "\n",
      "Epoch 37/200\n",
      "----------\n",
      "train Loss: 0.0256 Acc: 0.5368\n",
      "val Loss: 0.9652 Acc: 0.8043\n",
      "\n",
      "Epoch 38/200\n",
      "----------\n",
      "train Loss: 0.0258 Acc: 0.5365\n",
      "val Loss: 0.8218 Acc: 0.8168\n",
      "\n",
      "Epoch 39/200\n",
      "----------\n",
      "train Loss: 0.0248 Acc: 0.5369\n",
      "val Loss: 0.9214 Acc: 0.8106\n",
      "\n",
      "Epoch 40/200\n",
      "----------\n",
      "train Loss: 0.0273 Acc: 0.5363\n",
      "val Loss: 0.9664 Acc: 0.8054\n",
      "\n",
      "Epoch 41/200\n",
      "----------\n",
      "train Loss: 0.0252 Acc: 0.5369\n",
      "val Loss: 0.8963 Acc: 0.8066\n",
      "\n",
      "Epoch 42/200\n",
      "----------\n",
      "train Loss: 0.0240 Acc: 0.5370\n",
      "val Loss: 0.9287 Acc: 0.8106\n",
      "\n",
      "Epoch 43/200\n",
      "----------\n",
      "train Loss: 0.0226 Acc: 0.5374\n",
      "val Loss: 0.9241 Acc: 0.8054\n",
      "\n",
      "Epoch 44/200\n",
      "----------\n",
      "train Loss: 0.0245 Acc: 0.5365\n",
      "val Loss: 0.8169 Acc: 0.8230\n",
      "\n",
      "Epoch 45/200\n",
      "----------\n",
      "train Loss: 0.0244 Acc: 0.5368\n",
      "val Loss: 1.0455 Acc: 0.8009\n",
      "\n",
      "Epoch 46/200\n",
      "----------\n",
      "train Loss: 0.0254 Acc: 0.5365\n",
      "val Loss: 0.9315 Acc: 0.8015\n",
      "\n",
      "Epoch 47/200\n",
      "----------\n",
      "train Loss: 0.0244 Acc: 0.5371\n",
      "val Loss: 0.9078 Acc: 0.8236\n",
      "\n",
      "Epoch 48/200\n",
      "----------\n",
      "train Loss: 0.0253 Acc: 0.5365\n",
      "val Loss: 0.9868 Acc: 0.7952\n",
      "\n",
      "Epoch 49/200\n",
      "----------\n",
      "train Loss: 0.0242 Acc: 0.5365\n",
      "val Loss: 0.8895 Acc: 0.8185\n",
      "\n",
      "Epoch 50/200\n",
      "----------\n",
      "train Loss: 0.0245 Acc: 0.5366\n",
      "val Loss: 0.9360 Acc: 0.8060\n",
      "\n",
      "Epoch 51/200\n",
      "----------\n",
      "train Loss: 0.0249 Acc: 0.5368\n",
      "val Loss: 1.0191 Acc: 0.7947\n",
      "\n",
      "Epoch 52/200\n",
      "----------\n",
      "train Loss: 0.0244 Acc: 0.5367\n",
      "val Loss: 0.8382 Acc: 0.8247\n",
      "\n",
      "Epoch 53/200\n",
      "----------\n",
      "train Loss: 0.0234 Acc: 0.5371\n",
      "val Loss: 0.9705 Acc: 0.7969\n",
      "\n",
      "Epoch 54/200\n",
      "----------\n",
      "train Loss: 0.0247 Acc: 0.5366\n",
      "val Loss: 1.0064 Acc: 0.8009\n",
      "\n",
      "Epoch 55/200\n",
      "----------\n",
      "train Loss: 0.0236 Acc: 0.5372\n",
      "val Loss: 0.9929 Acc: 0.7930\n",
      "\n",
      "Epoch 56/200\n",
      "----------\n",
      "train Loss: 0.0229 Acc: 0.5378\n",
      "val Loss: 0.9460 Acc: 0.8066\n",
      "\n",
      "Epoch 57/200\n",
      "----------\n",
      "train Loss: 0.0248 Acc: 0.5364\n",
      "val Loss: 0.9301 Acc: 0.7986\n",
      "\n",
      "Epoch 58/200\n",
      "----------\n",
      "train Loss: 0.0237 Acc: 0.5369\n",
      "val Loss: 0.8482 Acc: 0.8151\n",
      "\n",
      "Epoch 59/200\n",
      "----------\n",
      "train Loss: 0.0229 Acc: 0.5373\n",
      "val Loss: 0.9193 Acc: 0.8134\n",
      "\n",
      "Epoch 60/200\n",
      "----------\n",
      "train Loss: 0.0245 Acc: 0.5366\n",
      "val Loss: 0.8964 Acc: 0.8043\n",
      "\n",
      "Epoch 61/200\n",
      "----------\n",
      "train Loss: 0.0239 Acc: 0.5368\n",
      "val Loss: 0.9997 Acc: 0.8009\n",
      "\n",
      "Epoch 62/200\n",
      "----------\n",
      "train Loss: 0.0251 Acc: 0.5361\n",
      "val Loss: 0.9553 Acc: 0.8162\n",
      "\n",
      "No Validation Accuracy decrease over the last 30 epochs. Stopping training...\n",
      "Training complete in 91m 16s\n",
      "Best val Acc: 0.827567\n",
      "Val Accuracy: 0.827566647759501\n",
      "Test Accuracy: -1\n"
     ]
    }
   ],
   "source": [
    "conv_path = \"models/92_classifier/ct_fp2ad_mixed.pt\"\n",
    "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 10)\n",
    "build_from_path(model_ft, conv_path, train_batch=8, test_batch=32, decay = True, epochs = 200, early_stop = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 1.0090 Acc: 0.7916\n"
     ]
    }
   ],
   "source": [
    "build_from_path(model_ft, conv_path, train_batch=8, test_batch=32, decay = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Mixed in Greater Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load already trained model\n",
    "model_ft =  models.convnext_tiny(pretrained = True)\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model_ft.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_ft.avgpool.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_ft.features[7].parameters():\n",
    "    param.requires_grad = True\n",
    "#Unfreeze the next convolutional block\n",
    "for param in model_ft.features[6].parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_ft.features[5].parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "# Create Last Layer\n",
    "num_ftrs = model_ft.classifier[2].in_features\n",
    "model_ft.classifier[2] = nn.Linear(num_ftrs, species_count)\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m conv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/92_classifier/ct_fpd_mixed_.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m exp_lr_scheduler \u001b[38;5;241m=\u001b[39m lr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(optimizer_ft, factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.2\u001b[39m, patience \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mbuild_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_ft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecay\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36mbuild_from_path\u001b[0;34m(model_ft, path, image_datasets, weighted_samplers, decay, train_batch, test_batch, epochs, early_stop)\u001b[0m\n\u001b[1;32m      9\u001b[0m     model_ft, val_acc, test_acc, epoch_accs \u001b[38;5;241m=\u001b[39m train_test_save_model(path, dataloaders, model_ft, criterion, optimizer_ft, exp_lr_scheduler, epochs, \u001b[38;5;28;01mTrue\u001b[39;00m, early_stop \u001b[38;5;241m=\u001b[39m early_stop)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 11\u001b[0m     model_ft, val_acc, test_acc, epoch_accs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_save_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_ft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_ft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_lr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mtrain_test_save_model\u001b[0;34m(save_path, dataloaders, model, criterion, optimizer, scheduler, num_epochs, verbose, plateau, test, early_stop)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_test_save_model\u001b[39m(save_path, dataloaders, model, criterion, optimizer, scheduler, num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m25\u001b[39m, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, plateau \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, early_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     trained_model, val_acc, epoch_accs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplateau\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      4\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), save_path)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m test:\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(dataloaders, model, criterion, optimizer, scheduler, num_epochs, verbose, plateau, early_stop)\u001b[0m\n\u001b[1;32m     47\u001b[0m             optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# statistics\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     51\u001b[0m     running_corrects \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(preds \u001b[38;5;241m==\u001b[39m labels\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m     53\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m dataset_sizes[phase]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "conv_path = \"models/92_classifier/ct_fpd_mixed_.pt\"\n",
    "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 5)\n",
    "build_from_path(model_ft, conv_path, train_batch=8, test_batch=32, decay = True, epochs = 30, early_stop = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_from_path(model_ft, conv_path, train_batch=8, test_batch=32, decay = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
