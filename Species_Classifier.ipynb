{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as ds\n",
    "from torchvision import models, transforms, utils, datasets\n",
    "from torch.utils import data\n",
    "from torchsummary import summary\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Read-In and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_df = pd.read_csv(\"data/fish_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_fish_df = pd.read_csv(\"data/is_fish.csv\").iloc[:,1:]\n",
    "is_fish_df[\"local_paths\"] = is_fish_df[\"Species\"].astype(str) + \"/\" + is_fish_df[\"Filename\"]\n",
    "path_set = set(is_fish_df[\"local_paths\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_count = 92 #len(is_fish_df[\"Species\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We normalize to imagenet mean for the data (https://stackoverflow.com/questions/58151507/why-pytorch-officially-use-mean-0-485-0-456-0-406-and-std-0-229-0-224-0-2)\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        transforms.Resize([224, 224])]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        transforms.Resize([224, 224])]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        transforms.Resize([224, 224])])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dirs = [\"/home/shivaram/DS/Projects/FishID/data/model_data/is_fish_scientific/\", \"/home/shivaram/DS/Projects/FishID/data/model_data/is_fish_common/\", \"/home/shivaram/DS/Projects/FishID/data/model_data/is_fish_mixed/\"]\n",
    "diff_image_datasets = {dd:{x: datasets.ImageFolder(os.path.join(dd, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val', 'test']} for dd in data_dirs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = diff_image_datasets[\"/home/shivaram/DS/Projects/FishID/data/model_data/is_fish_scientific/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64#8#32#64\n",
    "#epoch_samples = 2560# len(samples_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_samplers = {}\n",
    "for subset in [\"train\", \"val\", \"test\"]:\n",
    "    target = image_datasets[subset].targets\n",
    "    \n",
    "    if True:#subset == \"train\":\n",
    "        class_sample_count =np.array([ len(np.where(target == t)[0]) for t in np.unique(target)])\n",
    "        weight = 1. / class_sample_count\n",
    "        samples_weight = np.array([weight[t] for t in target])\n",
    "        samples_weight = torch.from_numpy(samples_weight)\n",
    "        epoch_samples = len(samples_weight)\n",
    "        sampler = data.WeightedRandomSampler(samples_weight, epoch_samples)\n",
    "        weighted_samplers[subset] = sampler\n",
    "    else:\n",
    "        sampler = data.RandomSampler(image_datasets[subset])\n",
    "        weighted_samplers[subset] = sampler\n",
    "    \n",
    "\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], \n",
    "                                              batch_size=batch_size, num_workers=4)\n",
    "              for x in ['train', 'val', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "class_names = image_datasets['train'].classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataloaders, model, criterion, optimizer, scheduler, num_epochs=25, verbose = True):\n",
    "    since = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    epoch_accs = []\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epa = {\"Epoch\": epoch}\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "            print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "            if phase == 'train':\n",
    "                scheduler.step(epoch_loss)\n",
    "\n",
    "            \n",
    "            epa[f'{phase} Loss'] = float(epoch_loss)\n",
    "            epa[f'{phase} Accuracy'] = float(epoch_acc)\n",
    "            if verbose:\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        if verbose:\n",
    "            print()\n",
    "            \n",
    "        epoch_accs.append(epa)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    if verbose:\n",
    "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    epoch_accs = pd.DataFrame(epoch_accs)\n",
    "    return model, best_acc, epoch_accs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, criterion, phase = \"test\", verbose = True):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    for inputs, labels in dataloaders[phase]:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    epoch_loss = running_loss / dataset_sizes[phase]\n",
    "    epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "    if verbose:\n",
    "        print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        \n",
    "    return epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_save_model(save_path, dataloaders, model, criterion, optimizer, scheduler, num_epochs = 25, verbose = True):\n",
    "    \n",
    "    trained_model, val_acc, epoch_accs = train_model(dataloaders, model, criterion, optimizer, scheduler, num_epochs, verbose) \n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    test_acc = test_model(trained_model, criterion)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Val Accuracy: {val_acc}\")\n",
    "        print(f\"Test Accuracy: {test_acc}\")\n",
    "        \n",
    "\n",
    "    return trained_model, val_acc, test_acc, epoch_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining the Best Transfer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce RTX 3060 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Frozen Layers\n",
    "### ResNet18 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 1.8165 Acc: 0.6646\n"
     ]
    }
   ],
   "source": [
    "resnet18_path = \"models/92_classifier/resnet18.pt\"\n",
    "\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "#for param in model_ft.parameters():\n",
    "#    param.requires_grad = False\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, species_count)\n",
    "model_ft = model_ft.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.1, momentum=0.9)\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "#exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 3)\n",
    "\n",
    "\n",
    "if os.path.exists(resnet18_path):\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=64, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft.load_state_dict(torch.load(resnet18_path))\n",
    "    test_model(model_ft, criterion)\n",
    "else:\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=128, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft, val_acc, test_acc, epoch_accs = train_test_save_model(resnet18_path, dataloaders, model_ft, criterion, optimizer_ft, exp_lr_scheduler, 30, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decaying Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "----------\n",
      "train Loss: 0.7934 Acc: 0.7780\n",
      "val Loss: 2.1038 Acc: 0.5547\n",
      "\n",
      "Epoch 2/30\n",
      "----------\n",
      "train Loss: 0.7267 Acc: 0.7999\n",
      "val Loss: 2.1710 Acc: 0.5752\n",
      "\n",
      "Epoch 3/30\n",
      "----------\n",
      "train Loss: 0.6402 Acc: 0.8217\n",
      "val Loss: 1.8922 Acc: 0.5916\n",
      "\n",
      "Epoch 4/30\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 7)\n",
    "resnet18_path = \"models/92_classifier/resnet18_decay.pt\"\n",
    "if os.path.exists(resnet18_path):\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=64, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft.load_state_dict(torch.load(resnet18_path))\n",
    "    test_model(model_ft, criterion)\n",
    "else:\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=128, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft, val_acc, test_acc, epoch_accs = train_test_save_model(resnet18_path, dataloaders, model_ft, criterion, optimizer_ft, exp_lr_scheduler, 30, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient Net B0\n",
    "Batch size for train 32 and batch size for eval 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m efficientnetb0_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/92_classifier/efficient_netb0.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m model_ft \u001b[38;5;241m=\u001b[39m  \u001b[43mmodels\u001b[49m\u001b[38;5;241m.\u001b[39mefficientnet_b0(pretrained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#for param in model_ft.parameters():\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#    param.requires_grad = False\u001b[39;00m\n\u001b[1;32m      6\u001b[0m num_ftrs \u001b[38;5;241m=\u001b[39m model_ft\u001b[38;5;241m.\u001b[39mclassifier[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39min_features\n",
      "\u001b[0;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "efficientnetb0_path = \"models/92_classifier/efficient_netb0.pt\"\n",
    "\n",
    "model_ft =  models.efficientnet_b0(pretrained = True)\n",
    "#for param in model_ft.parameters():\n",
    "#    param.requires_grad = False\n",
    "num_ftrs = model_ft.classifier[1].in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, species_count)\n",
    "model_ft = model_ft.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.1, momentum=0.9)\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "#exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 3)\n",
    "\n",
    "if os.path.exists(efficientnetb0_path):\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=8, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft.load_state_dict(torch.load(efficientnetb0_path))\n",
    "    test_model(model_ft, criterion)\n",
    "else:\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=32, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft, val_acc, test_acc, epoch_accs = train_test_save_model(efficientnetb0_path, dataloaders, model_ft, criterion, optimizer_ft, exp_lr_scheduler, 30, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decaying Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 7)\n",
    "efficientnetb0_path = \"models/92_classifier/efficient_netb0_decay.pt\"\n",
    "if os.path.exists(efficientnetb0_path):\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=8, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft.load_state_dict(torch.load(efficientnetb0_path))\n",
    "    test_model(model_ft, criterion)\n",
    "else:\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=32, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft, val_acc, test_acc, epoch_accs = train_test_save_model(efficientnetb0_path, dataloaders, model_ft, criterion, optimizer_ft, exp_lr_scheduler, 30, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convnet Tiny\n",
    "Batch size for train 32 and for test 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 1.3758 Acc: 0.7081\n"
     ]
    }
   ],
   "source": [
    "conv_path = \"models/92_classifier/conv_tiny.pt\"\n",
    "\n",
    "model_ft =  models.convnext_tiny(pretrained = True)\n",
    "#for param in model_ft.parameters():\n",
    "#    param.requires_grad = False\n",
    "num_ftrs = model_ft.classifier[2].in_features\n",
    "\n",
    "model_ft.fc = nn.Linear(num_ftrs, species_count)\n",
    "model_ft = model_ft.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.01, momentum=0.9)\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "#exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 3)\n",
    "\n",
    "if os.path.exists(conv_path):\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=8, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft.load_state_dict(torch.load(conv_path))\n",
    "    test_model(model_ft, criterion)\n",
    "else:\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=32, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft, val_acc, test_acc, epoch_accs = train_test_save_model(conv_path, dataloaders, model_ft, criterion, optimizer_ft, exp_lr_scheduler, 30, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decaying Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 7)\n",
    "conv_path = \"models/92_classifier/conv_tiny_decay.pt\"\n",
    "if os.path.exists(conv_path):\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=8, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft.load_state_dict(torch.load(conv_path))\n",
    "    test_model(model_ft, criterion)\n",
    "else:\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=32, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft, val_acc, test_acc, epoch_accs = train_test_save_model(conv_path, dataloaders, model_ft, criterion, optimizer_ft, exp_lr_scheduler, 30, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frozen Layers\n",
    "### ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18_path = \"models/92_classifier/rn18_frozen.pt\"\n",
    "\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, species_count)\n",
    "model_ft = model_ft.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.1, momentum=0.9)\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "#exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 7)\n",
    "\n",
    "\n",
    "if os.path.exists(resnet18_path):\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=64, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft.load_state_dict(torch.load(resnet18_path))\n",
    "    test_model(model_ft, criterion)\n",
    "else:\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=128, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft, val_acc, test_acc, epoch_accs = train_test_save_model(resnet18_path, dataloaders, model_ft, criterion, optimizer_ft, exp_lr_scheduler, 30, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient Net B0\n",
    "\n",
    "https://discuss.pytorch.org/t/how-the-pytorch-freeze-network-in-some-layers-only-the-rest-of-the-training/7088/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     test_model(model_ft, criterion)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 23\u001b[0m     model_ft, val_acc, test_acc, epoch_accs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_save_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mefficientnetb0_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_ft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_ft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_lr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mtrain_test_save_model\u001b[0;34m(save_path, dataloaders, model, criterion, optimizer, scheduler, num_epochs, verbose)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_test_save_model\u001b[39m(save_path, dataloaders, model, criterion, optimizer, scheduler, num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m25\u001b[39m, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m----> 3\u001b[0m     trained_model, val_acc, epoch_accs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      4\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), save_path)\n\u001b[1;32m      5\u001b[0m     test_acc \u001b[38;5;241m=\u001b[39m test_model(trained_model, criterion)\n",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(dataloaders, model, criterion, optimizer, scheduler, num_epochs, verbose)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m dataloaders[phase]:\n\u001b[1;32m     28\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 29\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device)\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# zero the parameter gradients\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "efficientnetb0_path = \"models/92_classifier/en_netb0_frozen_decay.pt\"\n",
    "\n",
    "model_ft =  models.efficientnet_b0(pretrained = True)\n",
    "\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "    \n",
    "num_ftrs = model_ft.classifier[1].in_features\n",
    "model_ft.classifier[1] = nn.Linear(num_ftrs, species_count)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.1, momentum=0.9)\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "#exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 10)\n",
    "\n",
    "if os.path.exists(efficientnetb0_path):\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=8, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft.load_state_dict(torch.load(efficientnetb0_path))\n",
    "    test_model(model_ft, criterion)\n",
    "else:\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=32, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft, val_acc, test_acc, epoch_accs = train_test_save_model(efficientnetb0_path, dataloaders, model_ft, criterion, optimizer_ft, exp_lr_scheduler, 30, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convnet Tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "----------\n",
      "train Loss: 2.2187 Acc: 0.4948\n",
      "val Loss: 1.9179 Acc: 0.5026\n",
      "\n",
      "Epoch 2/30\n",
      "----------\n",
      "train Loss: 1.5427 Acc: 0.6336\n",
      "val Loss: 1.7820 Acc: 0.5440\n",
      "\n",
      "Epoch 3/30\n",
      "----------\n",
      "train Loss: 1.3817 Acc: 0.6674\n",
      "val Loss: 1.6825 Acc: 0.5729\n",
      "\n",
      "Epoch 4/30\n",
      "----------\n",
      "train Loss: 1.2805 Acc: 0.6897\n",
      "val Loss: 1.6486 Acc: 0.5644\n",
      "\n",
      "Epoch 5/30\n",
      "----------\n",
      "train Loss: 1.2121 Acc: 0.7040\n",
      "val Loss: 1.5331 Acc: 0.5865\n",
      "\n",
      "Epoch 6/30\n",
      "----------\n",
      "train Loss: 1.1622 Acc: 0.7164\n",
      "val Loss: 1.5343 Acc: 0.6075\n",
      "\n",
      "Epoch 7/30\n",
      "----------\n",
      "train Loss: 1.1288 Acc: 0.7243\n",
      "val Loss: 1.4792 Acc: 0.6092\n",
      "\n",
      "Epoch 8/30\n",
      "----------\n",
      "train Loss: 1.0999 Acc: 0.7331\n",
      "val Loss: 1.5007 Acc: 0.6018\n",
      "\n",
      "Epoch 9/30\n",
      "----------\n",
      "train Loss: 1.0741 Acc: 0.7420\n",
      "val Loss: 1.6164 Acc: 0.5871\n",
      "\n",
      "Epoch 10/30\n",
      "----------\n",
      "train Loss: 1.0781 Acc: 0.7377\n",
      "val Loss: 1.4610 Acc: 0.6149\n",
      "\n",
      "Epoch 11/30\n",
      "----------\n",
      "train Loss: 1.0600 Acc: 0.7458\n",
      "val Loss: 1.5462 Acc: 0.6024\n",
      "\n",
      "Epoch 12/30\n",
      "----------\n",
      "train Loss: 1.0568 Acc: 0.7443\n",
      "val Loss: 1.4919 Acc: 0.6166\n",
      "\n",
      "Epoch 13/30\n",
      "----------\n",
      "train Loss: 1.0669 Acc: 0.7450\n",
      "val Loss: 1.4286 Acc: 0.6149\n",
      "\n",
      "Epoch 14/30\n",
      "----------\n",
      "train Loss: 1.0496 Acc: 0.7465\n",
      "val Loss: 1.4866 Acc: 0.6126\n",
      "\n",
      "Epoch 15/30\n",
      "----------\n",
      "train Loss: 1.0546 Acc: 0.7443\n",
      "val Loss: 1.5734 Acc: 0.5939\n",
      "\n",
      "Epoch 16/30\n",
      "----------\n",
      "train Loss: 1.0649 Acc: 0.7425\n",
      "val Loss: 1.5264 Acc: 0.6041\n",
      "\n",
      "Epoch 17/30\n",
      "----------\n",
      "train Loss: 1.0577 Acc: 0.7482\n",
      "val Loss: 1.5249 Acc: 0.5995\n",
      "\n",
      "Epoch 18/30\n",
      "----------\n",
      "train Loss: 1.0479 Acc: 0.7484\n",
      "val Loss: 1.5348 Acc: 0.5939\n",
      "\n",
      "Epoch 19/30\n",
      "----------\n",
      "train Loss: 1.0644 Acc: 0.7438\n",
      "val Loss: 1.5499 Acc: 0.6035\n",
      "\n",
      "Epoch 20/30\n",
      "----------\n",
      "train Loss: 1.0647 Acc: 0.7447\n",
      "val Loss: 1.5113 Acc: 0.5990\n",
      "\n",
      "Epoch 21/30\n",
      "----------\n",
      "train Loss: 1.0604 Acc: 0.7445\n",
      "val Loss: 1.4322 Acc: 0.6029\n",
      "\n",
      "Epoch 22/30\n",
      "----------\n",
      "train Loss: 1.0573 Acc: 0.7477\n",
      "val Loss: 1.5286 Acc: 0.6018\n",
      "\n",
      "Epoch 23/30\n",
      "----------\n",
      "train Loss: 1.0471 Acc: 0.7511\n",
      "val Loss: 1.5835 Acc: 0.6012\n",
      "\n",
      "Epoch 24/30\n",
      "----------\n",
      "train Loss: 1.0539 Acc: 0.7453\n",
      "val Loss: 1.4719 Acc: 0.5995\n",
      "\n",
      "Epoch 25/30\n",
      "----------\n",
      "train Loss: 1.0518 Acc: 0.7501\n",
      "val Loss: 1.5628 Acc: 0.6018\n",
      "\n",
      "Epoch 26/30\n",
      "----------\n",
      "train Loss: 1.0420 Acc: 0.7505\n",
      "val Loss: 1.4374 Acc: 0.6319\n",
      "\n",
      "Epoch 27/30\n",
      "----------\n",
      "train Loss: 1.0530 Acc: 0.7475\n",
      "val Loss: 1.4632 Acc: 0.6200\n",
      "\n",
      "Epoch 28/30\n",
      "----------\n",
      "train Loss: 1.0647 Acc: 0.7462\n",
      "val Loss: 1.3960 Acc: 0.6183\n",
      "\n",
      "Epoch 29/30\n",
      "----------\n",
      "train Loss: 1.0466 Acc: 0.7493\n",
      "val Loss: 1.5403 Acc: 0.6115\n",
      "\n",
      "Epoch 30/30\n",
      "----------\n",
      "train Loss: 1.0479 Acc: 0.7495\n",
      "val Loss: 1.5379 Acc: 0.6018\n",
      "\n",
      "Training complete in 37m 48s\n",
      "Best val Acc: 0.631877\n",
      "test Loss: 1.5365 Acc: 0.5978\n",
      "Val Accuracy: 0.6318774815655134\n",
      "Test Accuracy: 0.5977715877437326\n"
     ]
    }
   ],
   "source": [
    "conv_path = \"models/92_classifier/conv_tiny_frozen.pt\"\n",
    "\n",
    "model_ft =  models.convnext_tiny(pretrained = True)\n",
    "\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "num_ftrs = model_ft.classifier[2].in_features\n",
    "\n",
    "# Greate Last Layer\n",
    "model_ft.classifier[2] = nn.Linear(num_ftrs, species_count)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.01, momentum=0.9)\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "#exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor = .2, patience = 3)\n",
    "\n",
    "if os.path.exists(conv_path):\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=8, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft.load_state_dict(torch.load(conv_path))\n",
    "    test_model(model_ft, criterion)\n",
    "else:\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], sampler = weighted_samplers[x], batch_size=32, num_workers=4) for x in ['train', 'val', 'test']}\n",
    "    model_ft, val_acc, test_acc, epoch_accs = train_test_save_model(conv_path, dataloaders, model_ft, criterion, optimizer_ft, exp_lr_scheduler, 30, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo\n",
    "- Train Standard Network\n",
    "- Adjust sample weighting\n",
    "- Train Siamese Network or GAN\n",
    "- Refactor code\n",
    "- [Don't weighted random sample for validation and test](https://discuss.pytorch.org/t/dataloader-using-subsetrandomsampler-and-weightedrandomsampler-at-the-same-time/29907/7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
